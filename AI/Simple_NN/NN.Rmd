---
title: '6. Neural Network: Bacteria Classification'
author: "Garth Mortensen"
date: "October 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<center>

![](/Users/G/Google Drive/aStThomas/7ArtificalIntelligence/Assignments/2 NN/splash.jpg)

Image credit: <a href="https://unsplash.com/@cheaousa" target="_blank">Ousa Chea</a>.

</center>

This analysis builds a Neural Network (NN) to classify bacteria cell DNA into two groups; *Flagged,* and *Not Flagged* for further study. The dataset consists of 14 columns and 1,217 observations. The target label is scaled 0 to 10, where 0 is not flagged for further study.

NNs are stacks of successive learning *layers* of increasingly meaningful represenations (Deep Learning with Python, 8). You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (9). 

What makes NNs different is their improved performance on various problems, and simplification of problem solving since it automated the feature engineering step.

### Overview

Perform an analysis on the cellDNA dataset using a Neural Network to predict which observations are worthy of further study, given 13 features.

### Pre-Modeling

Neural Networks can require heavy computation, making them sometimes unfeasible for dealing with larger datasets. We'll measure how long this calculation takes, using a stopwatch method. This computer is running an i7-65000 with 16GB DDR4-2400 RAM. You can use <a href="https://www.cpuid.com/softwares/cpu-z.html" target="_blank">CPU-Z</a> to determine your RAM speed.

Start the stopwatch!
```{r}

start_time <- Sys.time()
```

#### Load Required Packages

1) Install pacman using the following code.
```{r}

#After experiencing my repository referencing the wrong mirror several times now, I keep this code available to avoid issues, especially when using knitr.
#local({r <- getOption("repos")
#       r["CRAN"] <- "http://cran.r-project.org" 
#       options(repos=r)
#})

# Load required packages
if (!require("pacman")) install.packages("pacman")

# If the above doesn't work, use the following. pacman is used for installing packages.
# #install.packages("pacman")
# library("pacman")
```

#### Load Data

The dataset we will be loading appears as:

![*Document Preview*](/Users/G/Google Drive/aStThomas/7ArtificalIntelligence/Assignments/2 NN/datapreview.png)

```{r}

cellDNA <- read.csv("C:/tmp/CellDNA.csv", header = TRUE)
```

#### Preview Data

Examine the data structure.

```{r}

#Preview structure
str(cellDNA)
```

Examine the top 5 rows.

```{r}

#Preview top 5 rows
head(cellDNA, n=5)
```

#### Preprocessing

The dataset is raw and requires some tidying up. We'll give the dataset header names and drop the ID column, which carries no useful information.

```{r}

#give col names
colnames(cellDNA) <- c("X1","X2", "X3", "X4","X5", "X6", "X7","X8", "X9", "X10","X11", "X12", "X13","Y")
```

We are going to give a binary classification to the Y target variable now, where anything other than 0 will equal to 1. The value 0 indicates a bacterium worthy of further study.

```{r}

#ifelse(test, true, false)
cellDNA$Y <- ifelse(cellDNA$Y>0, 1, 0)
```

Because all measurements in our dataframe are in different units, we need to standardize. There are no dummy variables included.

```{r}

#standardize all X variables
tempdf <- scale(cellDNA[,1:13])

#Bring Y back in
cellDNA = cbind(tempdf, cellDNA[,14])

#And reappoint column names
colnames(cellDNA) <- c("X1","X2", "X3", "X4","X5", "X6", "X7","X8", "X9", "X10","X11", "X12", "X13","Y")
```

### Data Split

In machine learning, we feed data to the model, which then finds the statistical structure and builds the defining rules. We don't feed all the data to the model up front though. We split our dataframe into a training and test observations. We give the training data to the model to find an underlying statistical structure. Once the model has defined the rules, then we run them over the test data, and see how it's performed. Here, we split the data 70-30 into training and test data, using the caret package, which randomizes the split by default. By setting the random seed, we can all work to make our studies [reproducable](https://en.wikipedia.org/wiki/Replication_crisis) for others.

```{r}
pacman::p_load(caret)
library(caret)

# https://en.wikipedia.org/wiki/42_(number)#The_Hitchhiker's_Guide_to_the_Galaxy
set.seed(42)

#By default, createDataPartition does a stratified random split
#https://topepo.github.io/caret/data-splitting.html
trainIndex <- createDataPartition(
  #define y
  cellDNA[,14], 
  #training set %
  p = 0.7, 
  #results in a list (TRUE) or a matrix
  list = FALSE, 
  #the number of partitions to create
  times = 1)

head(trainIndex)
```

The above outputs an index. We now use this index to split the dataset.

```{r}

training <- cellDNA[ trainIndex,]
testing  <- cellDNA[-trainIndex,]
```

The data has been split, and we can move onto the modeling phase.

### Modeling

We'll now use the neuralnet package and neuralnet algorithm to build a NN. Of note, this algorithm does not accept ~. notation (which is equivalent to "= all factors"). Instead, a verbose formula must be used.

```{r}

pacman::p_load(neuralnet)
library(neuralnet)

#start watch fit
start_time_fit <- Sys.time()

fit <- neuralnet(Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11+ X12 + X13, data = training, hidden=c(2), linear.output=FALSE, threshold=0.01)

#stop watch fit
end_time_fit <- Sys.time()
total_time_fit <- end_time_fit - start_time_fit
total_time_fit = round(total_time_fit, digits = 2)
```

We decided on using a neural net configuration of 2, meaning two neurons in a single hidden layer. A linear relationship between the independent and dependent (target) variables is assumed to be false, and parameterized as such. Finally, if error changes during an iteration less than 1%, then optimization stops (threshold = 0.01)

```{r}
#Print the results using the following
#fit$result.matrix
plot(fit)
```
![](/Users/G/Google Drive/aStThomas/7ArtificalIntelligence/Assignments/2 NN/NN_image.png)

Above we see a visual of our neural network. Black lines show connections with weights, which are calculated using back propogation. Blue lines display the bias term.

The above is difficult to read, especially when more inputs and nodes are used. To see the amount of error this fit does not explain, along with the determined weights between the inputs, hidden layers and output, use the following command.

```{r}

fit$result.matrix
```

### Validation

#### Training dataset

With our NN fitted, our next question is: how did it model the training data? We answer this by checking it's confusion matrix. Unlike Support Vector Machines, Decision Trees, Lasso Regression Models, etc, R's predict function will not work for NNs. Instead, we adopt a new method. For these steps, I closely followed this [guide](http://www.michaeljgrogan.com/neural-network-modelling-neuralnet-r/).

```{r}

# We subset to remove Y from the dataset
temp_training <- subset(training, select = c("X1","X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13"))

# Compute function creates the prediction variable
# It computes the outputs of all neurons for specific arbitrary covariate vectors given a trained neural network.
# compute(x, covariate, rep = 1), where:
# x	= an object of class nn.
# covariate	= a dataframe or matrix containing the variables that had been used to train the neural network.
fit.results <- compute(fit, temp_training)
```

Having found our fit results, we can build a new dataframe containing the actual and predicted results. The new dataframe will consist of the actual Y column from testing, and the predicted column from fit.results. We title these two columns as actual and prediction, and print the first 10 rows.

```{r}

results <- data.frame(actual = training[,14], prediction = fit.results$net.result)

# Display first 5 rows
results[1:10,]
```

We can pivot these columns into a confusion matrix. Pivot seems like a fun way to express this. But before we do that, notice that our predicted values are not 0s and 1s. We need to round them to their closest integer.

##### Accuracy, Precision, Recall, FScore

```{r}

rounded <- sapply(results, round, digits = 0)
rounded = data.frame(rounded) #redefine as df

# Is this needed?
attach(rounded)

conf <- table(actual, prediction)
```


```{r}

TP <- conf[1,1]
TN <- conf[2,2]
FP <- conf[1,2]
FN <- conf[2,1]

Accuracy <- round((TP + TN) / (TP + TN + FP + FN), digits = 3)
Precision <- round((TP / (TP + FP)), digits = 3)
Recall <- round((TP / (TP + FN)), digits = 3)
FScore <- round((2 * Precision * Recall)/(Precision + Recall), digits = 3)

train_Accuracy <- Accuracy
train_Precision <- Precision
train_Recall <- Recall
train_FScore <- FScore
```

On the training dataset, the NN correctly predicted `r TP` true positives and `r TN` true negatives. It missed by `r FP` with the false positives and `r FN` with the false negatives. 
The model's accuracy came to `r train_Accuracy`, precision `r train_Precision`, recall `r train_Recall` and FScore reached `r train_FScore`.

Are you pleased with the result? If your answer was either Yes or No, then you are wrong. The correct response would be Perhaps. It's simply too soon to say, since we haven't yet considered whether this model has been overfit. We need to see how the model performs on the test dataset. Our current parameters (weights) used to model the training dataset may not perform well on out-of-sample observations. To counter overfitting with NNs, we should provide more observations and adjust the model's threshold. 

#### Testing dataset

Having explored the models fit to the training dataset, we now explore its ability to generalize to the test dataset. This is merely repeating the above steps above, but replacing the dataset.

```{r}

# We subset to remove Y from the dataset
temp_testing <- subset(testing, select = c("X1","X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13"))

fit.results <- compute(fit, temp_testing)

results <- data.frame(actual = testing[,14], prediction = fit.results$net.result)

# Display first 5 rows
results[1:10,]

```

##### Accuracy, Precision, Recall, FScore

```{r}

rounded <- sapply(results, round, digits = 0)
rounded = data.frame(rounded) #redefine as df

# Is this needed?
attach(rounded)

conf <- table(actual, prediction)

TP <- conf[1,1]
TN <- conf[2,2]
FP <- conf[1,2]
FN <- conf[2,1]

Accuracy <- round((TP + TN) / (TP + TN + FP + FN), digits = 3)
Precision <- round((TP / (TP + FP)), digits = 3)
Recall <- round((TP / (TP + FN)), digits = 3)
FScore <- round((2 * Precision * Recall)/(Precision + Recall), digits = 3)

test_Accuracy <- Accuracy
test_Precision <- Precision
test_Recall <- Recall
test_FScore <- FScore
```

On the test set, the NN correctly predicted `r TP` true positives and `r TN` true negatives. It missed by `r FP` with the false positives and `r FN` with the false negatives. 

The model's accuracy came to `r test_Accuracy`, precision `r test_Precision`, recall `r test_Recall` and FScore reached `r test_FScore`.

##### Training vs Testing

**Accuracy**  
train_Accuracy = `r test_Accuracy`  
test_Accuracy = `r train_Accuracy`  
$\Delta$ = `r train_Accuracy - test_Accuracy`

**Precision**  
train_Precision = `r test_Precision`  
test_Precision = `r train_Precision`  
$\Delta$ = `r train_Precision - test_Precision`

**Recall**  
train_Recall = `r test_Recall`  
test_Recall = `r train_Recall`  
$\Delta$ = `r train_Recall - test_Recall`

**FScore**  
train_FScore = `r test_FScore`  
test_FScore = `r train_FScore`  
$\Delta$ = `r train_FScore - test_FScore`

### ROC Curve
*Accuracy increased??????????????*

*How calculate RMSE???????????*

*How do I determine Accuracy, etc for both classes??????????*

*Finally, we'll construct an ROC curve??????????*

### Results
*INCOMPLETE*
From the results, we see that 1,032 bacterium were *Flagged*, and 184 were *Not Flagged* for further study. This is slightly moved from our original dataset where 1,017 were *Flagged*, and 200 were *Not Flagged* for further study. 

#### Stopwatch

Click the stopwatch off.

```{r}

#stop watch
end_time <- Sys.time()
total_time <- end_time - start_time
total_time = round(total_time, digits = 2)
```

The total time required to process all rows into two classes was `r total_time` seconds. Of that total time, `r total_time_fit` seconds was used to fit the model.

Thank you for reading, and happy Neural Networking!

### Data

If you don't have the dataset, copy the table below, and after pasting it into Excel, save it as a comma separated file named patients.csv in your preferred directory.

```{r, echo = FALSE}

pacman::p_load(kableExtra)

kable(cellDNA) %>%
  kable_styling(bootstrap_options = c("striped", font_size = 7, "striped", "hover", "condensed", "responsive"))
```

### Contact
Email: <a href="mailto:mortensengarth@hotmail.com?Subject=R%20Code" target="_top">mortensengarth@hotmail.com</a>

LinkedIn: <a href="https://www.linkedin.com/in/mortensengarth/" target="_blank">https://www.linkedin.com/in/mortensengarth/</a>.