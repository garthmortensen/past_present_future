{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Representation\n",
    "## Text Analysis, Take 2\n",
    "Garth Mortensen  \n",
    "2019.09.19\n",
    "\n",
    "## Overview\n",
    "\n",
    "This analysis builds upon cleaning.ipynb, by shifting from [Project Gutenberg](http://www.gutenberg.org/) to [Pubmed](https://www.ncbi.nlm.nih.gov/pubmed/) as a source for text. In doing so, I now employ beautifulsoup library to target text appearing in the Abstract element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text\n",
    "\n",
    "Define two conditions of interest, and construct top 20 URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL: ['https://www.ncbi.nlm.nih.gov/pubmed/?term=diabetes%5BJOUR%5D', 'https://www.ncbi.nlm.nih.gov/pubmed/?term=cancer%5BJOUR%5D']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "conditions = [\"diabetes\", \"cancer\"]\n",
    "\n",
    "condition_urls = []\n",
    "\n",
    "\n",
    "def construction_url(condition_list):\n",
    "\n",
    "    for each_condition_list in condition_list:\n",
    "        \n",
    "        journal = \"%5BJOUR%5D\"  # 5D for most recent, I think.\n",
    "        condition_url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=\" + each_condition_list + journal\n",
    "        condition_urls.append(condition_url)\n",
    "\n",
    "    return condition_urls\n",
    "\n",
    "\n",
    "construction_url_output = construction_url(conditions)\n",
    "print(\"INITIAL:\", construction_url_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 20 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 7, 4, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "listy = [1, 4, 2, 4, 6, 7, 4, 2, 3]\n",
    "\n",
    "  \n",
    "# printing modified list \n",
    "print(listy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_20_articles(get_20_articles_input):\n",
    "\n",
    "    html = urlopen(get_20_articles_input).read()\n",
    "\n",
    "    # parse the html using beautiful soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    all_articles = []\n",
    "    url_start = \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "\n",
    "    articles = soup.find_all(attrs={'class': 'title'})\n",
    "    # print(articles)\n",
    "\n",
    "    # process main, starter page, and extract list of urls\n",
    "    for article in articles:\n",
    "\n",
    "        # split by uid, which is article id\n",
    "        link = str(article).split('uid=')\n",
    "        # print(link[1])\n",
    "\n",
    "        # remove anything after the uid, and define the number\n",
    "        uid = str(link[1]).split('&amp')[0]\n",
    "        # print(uid)\n",
    "\n",
    "        # build the complete url\n",
    "        url = url_start + uid\n",
    "\n",
    "        all_articles.append(url)\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "\n",
    "get_20_articles_output_0 = get_20_articles(construction_url_output[0])\n",
    "get_20_articles_output_1 = get_20_articles(construction_url_output[1])\n",
    "\n",
    "unwanted_0 = [7]\n",
    "for ele in sorted(unwanted_0, reverse = True):\n",
    "    del get_20_articles_output_0[ele]\n",
    "\n",
    "unwanted_1 = [0, 1, 8, 13, 19]\n",
    "for ele in sorted(unwanted_1, reverse = True):\n",
    "    del get_20_articles_output_1[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.ncbi.nlm.nih.gov/pubmed/31582408', 'https://www.ncbi.nlm.nih.gov/pubmed/31578193', 'https://www.ncbi.nlm.nih.gov/pubmed/31578192']\n"
     ]
    }
   ],
   "source": [
    "url_start = \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "url_list = [\"31582408\",\"31578193\",\"31578192\",\"31540941\",\"31540940\",\"31540938\",\"31420409\",\"31399432\",\"31391173\",\"31391172\",\"31375510\",\"31371518\",\"31350303\",\"31345937\",\"31337616\",\"31311801\",\"31311800\",\"31292135\",\"31292134\",\"31537525\",\"31537524\",\"31530579\",\"31519700\",\"31519699\",\"31506345\",\"31506344\",\"31506343\",\"31492662\",\"31492661\",\"31431441\",\"31431440\",\"31217176\",\"31217175\",\"31217174\",\"31201281\",\"31201280\",\"31175102\",\"31175101\",\"31175100\",\"31171562\",\"31167879\",\"31167878\",\"31167877\",\"30862679\",\"31471292\",\"31451517\",\"31439648\",\"31439647\",\"31439646\",\"31439645\",\"31439644\",\"31439643\",\"31439642\",\"31439641\",\"31399433\",\"31399431\",\"31331989\",\"31331988\",\"31331987\",\"31178433\",\"31167880\",\"31127058\",\"31127057\",\"31127056\",\"31127055\",\"31127053\",\"31127052\",\"31092480\",\"31092478\",\"31088856\",\"31088855\",\"31088854\",\"31221802\",\"31221801\",\"31221800\",\"31127054\",\"31092479\",\"31092477\",\"31048370\",\"31048369\",\"31048368\",\"31048367\",\"31010960\",\"31010959\",\"31010958\",\"31010957\",\"31010956\",\"31010955\",\"30962220\",\"30962219\",\"31178432\",\"31109941\",\"31109940\",\"31109939\",\"31109938\",\"30967424\",\"30936150\",\"30936149\",\"30936148\",\"30936147\",\"30936146\",\"30936145\",\"30936144\",\"30936143\",\"30936142\",\"30936140\",\"30894367\",\"30894366\",\"30885990\",\"30885989\",\"30862683\",\"30862682\",\"30862681\",\"30862680\",\"31010883\",\"31010882\",\"31010881\",\"31010880\",\"31010879\",\"31010878\",\"31010877\",\"30936151\",\"30936141\",\"30862678\",\"30833470\",\"30833469\",\"30833468\",\"30833467\",\"30833466\",\"30833465\",\"30796029\",\"30796028\",\"30765337\",\"30765336\",\"30765335\",\"30733330\",\"30728185\",\"30626611\",\"30455375\",\"30894392\",\"30755400\",\"30745441\",\"30728184\",\"30728183\",\"30696708\",\"30692245\",\"30679185\",\"30679184\",\"30679183\",\"30674623\",\"30674622\",\"30670477\",\"30655386\",\"30655385\",\"30626610\",\"30626607\",\"30617218\",\"30787070\",\"30787069\",\"30787068\",\"30787067\",\"30787066\",\"30635275\",\"30635274\",\"30626609\",\"30626608\",\"30617219\",\"30552111\",\"30552110\",\"30552109\",\"30552108\",\"30552107\",\"30523028\",\"30523027\",\"30523026\",\"30523025\",\"30523024\",\"30530781\",\"30409780\",\"30305367\",\"31069088\",\"30796027\",\"30816853\",\"30816852\",\"30665957\",\"30665956\",\"30665955\",\"30665954\",\"30665953\",\"30665952\",\"30665951\",\"30552106\",\"30487265\",\"30487264\",\"30487263\",\"30487262\",\"30455377\",\"30455376\",\"30425064\",\"30425063\"]\n",
    "\n",
    "all_urls = []\n",
    "\n",
    "for url in url_list:\n",
    "    entire_url = url_start + url\n",
    "    all_urls.append(entire_url)\n",
    "\n",
    "print(all_urls[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-acad42a6de1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mparseURL_output_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_20_articles_output_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mparseURL_output_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_20_articles_output_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-acad42a6de1e>\u001b[0m in \u001b[0;36mparseURL\u001b[0;34m(url_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Point to the element of interest on pubmed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mname_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'abstr'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# remove Abstract from title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "def parseURL(url_list):\n",
    "    \"\"\"Pull Abstract from PubMed article.\"\"\"\n",
    "\n",
    "    all_text = []\n",
    "    \n",
    "    for each_url in url_list:\n",
    "    \n",
    "        html = urlopen(each_url).read()\n",
    "\n",
    "        # parse the html using beautiful soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Point to the element of interest on pubmed\n",
    "        name_box = soup.find(attrs={'class': 'abstr'})\n",
    "        text = name_box.text.strip()\n",
    "\n",
    "        # remove Abstract from title\n",
    "        text = text[8:]\n",
    "\n",
    "        # remove everything after.\n",
    "        split_text = \"Copyright\"\n",
    "\n",
    "        # split on beginning of unwanted text, and take first half\n",
    "        text = text.split(split_text)[0]\n",
    "\n",
    "        # remove everything after.\n",
    "        split_text = \"©\"\n",
    "\n",
    "        # remove after copyright symbole of © American Diabetes Association\n",
    "        text = text.split(split_text)[0]\n",
    "\n",
    "        all_text.append(text)\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "\n",
    "parseURL_output_0 = parseURL(get_20_articles_output_0)\n",
    "parseURL_output_1 = parseURL(get_20_articles_output_1)\n",
    "\n",
    "print(type(parseURL_output_0))\n",
    "\n",
    "n = 0\n",
    "\n",
    "for each_parseURL_output in parseURL_output_0:\n",
    "    n += 1\n",
    "    print(str(n) + \":\", len(each_parseURL_output))\n",
    "    print(each_parseURL_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "Because the text being analyzed in this notebook are pubmed health studies, they are technical langauge. Many numbers, statistics symbols and health-space vocabulary words are used. We should be mindful of what text cleaning operations are applied.\n",
    "\n",
    "Do various things such as remove punctuation, remove whitespace, etc.\n",
    "\n",
    "One goal during text cleanup is to remove total word space, so that our resultant vectors are smaller. Think regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 121\n",
      "2: 145\n",
      "3: 87\n",
      "['novel', 'biomarkers', 'type', 'diabetes', 'and', 'response', 'preventative', 'treatment', 'individuals', 'with', 'similar', 'clinical', 'risk', 'may', 'highlight', 'metabolic', 'pathways', 'that', 'are', 'important', 'disease', 'profiled', 'metabolites', 'baseline', 'plasma', 'samples', 'from', 'the', 'diabetes', 'prevention', 'program', 'cox', 'models', 'were', 'used', 'determine', 'associations', 'between', 'metabolites', 'and', 'incident', 'well', 'whether', 'associations', 'differed', 'treatment', 'group', 'lifestyle', 'metformin', 'placebo', 'over', 'average', 'years', 'follow', 'found', 'metabolites', 'associated', 'with', 'incident', 'regardless', 'treatment', 'cytosine', 'was', 'novel', 'and', 'associated', 'with', 'the', 'lowest', 'exploratory', 'baseline', 'metabolite', 'associations', 'with', 'incident', 'differed', 'across', 'the', 'treatment', 'stratification', 'baseline', 'levels', 'several', 'these', 'including', 'specific', 'phospholipids', 'and', 'adenosine', 'monophosphate', 'modified', 'the', 'effect', 'that', 'ils', 'met', 'had', 'diabetes', 'our', 'findings', 'highlight', 'novel', 'markers', 'diabetes', 'risk', 'and', 'preventive', 'treatment', 'effect', 'individuals', 'that', 'are', 'clinically', 'high', 'risk', 'and', 'motivate', 'further', 'studies', 'validate', 'these']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(articles_content):\n",
    "    \"\"\"Remove unwanted characters, etc from articles\"\"\"\n",
    "    \n",
    "    articles_content_stripped = []\n",
    "    \n",
    "    for each_articles_content in articles_content:\n",
    "\n",
    "        # lower the article\n",
    "        each_articles_content = each_articles_content.lower()\n",
    "        \n",
    "        # split by white spaces\n",
    "        words = each_articles_content.split()\n",
    "\n",
    "        # remove all tokens that are not alphabetic\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "\n",
    "        # remove words < char len 2 and > 15\n",
    "        words = [word for word in words if len(word) > 2 and len(word) < 15]\n",
    "        \n",
    "        articles_content_stripped.append(words)\n",
    "        \n",
    "    return articles_content_stripped\n",
    "\n",
    "\n",
    "clean_text_output_0 = clean_text(parseURL_output_0)\n",
    "clean_text_output_1 = clean_text(parseURL_output_1)\n",
    "\n",
    "n = 0\n",
    "\n",
    "for each_clean_text_output in clean_text_output_0:\n",
    "    n += 1\n",
    "    print(str(n) + \":\", len(each_clean_text_output))\n",
    "\n",
    "print(clean_text_output_0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "\n",
    "The words we will remove are shown below.\n",
    "\n",
    "We can easily create our own customized stopword list, so as to remove any common, low-information medical terminology. It is just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total English stopwords: 179\n",
      "1: 92\n",
      "2: 100\n",
      "3: 69\n",
      "['novel', 'biomarkers', 'type', 'diabetes', 'response', 'preventative', 'treatment', 'individuals', 'similar', 'clinical', 'risk', 'may', 'highlight', 'metabolic', 'pathways', 'important', 'disease', 'profiled', 'metabolites', 'baseline', 'plasma', 'samples', 'diabetes', 'prevention', 'program', 'cox', 'models', 'used', 'determine', 'associations', 'metabolites', 'incident', 'well', 'whether', 'associations', 'differed', 'treatment', 'group', 'lifestyle', 'metformin', 'placebo', 'average', 'years', 'follow', 'found', 'metabolites', 'associated', 'incident', 'regardless', 'treatment', 'cytosine', 'novel', 'associated', 'lowest', 'exploratory', 'baseline', 'metabolite', 'associations', 'incident', 'differed', 'across', 'treatment', 'stratification', 'baseline', 'levels', 'several', 'including', 'specific', 'phospholipids', 'adenosine', 'monophosphate', 'modified', 'effect', 'ils', 'met', 'diabetes', 'findings', 'highlight', 'novel', 'markers', 'diabetes', 'risk', 'preventive', 'treatment', 'effect', 'individuals', 'clinically', 'high', 'risk', 'motivate', 'studies', 'validate']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"\\ntotal English stopwords:\", len(stop_words))\n",
    "# print(stop_words)\n",
    "\n",
    "\n",
    "def remove_stopwords(input_clean_text_output):\n",
    "    \"\"\"Remove stopwords\"\"\"\n",
    "    \n",
    "    all_texts_w_stopwords_removed = []\n",
    "    \n",
    "    for each_input_clean_text_output in input_clean_text_output:\n",
    "\n",
    "        keep_words = [w for w in each_input_clean_text_output if not w in stop_words]\n",
    "   \n",
    "        all_texts_w_stopwords_removed.append(keep_words)\n",
    "\n",
    "    return all_texts_w_stopwords_removed\n",
    "\n",
    "\n",
    "remove_stopwords_output_0 = remove_stopwords(clean_text_output_0)\n",
    "remove_stopwords_output_1 = remove_stopwords(clean_text_output_1)\n",
    "\n",
    "n = 0\n",
    "\n",
    "for each_remove_stopwords_output in remove_stopwords_output_0:\n",
    "    n += 1\n",
    "    print(str(n) + \":\", len(each_remove_stopwords_output))\n",
    "\n",
    "print(remove_stopwords_output_0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "According to [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) \n",
    ">**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "Porter's stemmer is the most well known stemmer. Time and place = Cambridge (Oxford), 1980. There are stemmers for other languages that follow other rules. It removes the common morphological and inflextional endings from English words. Its main usei s as part of a term normalization process that is usually done when setting up Information Retrieval systems.\n",
    "\n",
    "Word -> Stemmer -> Root form\n",
    "\n",
    "The goal is to reduce the total vocabulary space by simplifying words into their stems, chopping off the suffix.\n",
    "\n",
    "_cat_ <- car, cats, cat's, cats'  \n",
    "I suspect that Old English/strong/irregular verbs tend to be harder to stem.\n",
    "Do, did, done\n",
    "\n",
    "Weak/regular/latin verbs should be easier to stem, as the suffix determines their tense.\n",
    "Yell, yelled, yelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['novel', 'biomark', 'type', 'diabet', 'respons', 'prevent', 'treatment', 'individu', 'similar', 'clinic', 'risk', 'may', 'highlight', 'metabol', 'pathway', 'import', 'diseas', 'profil', 'metabolit', 'baselin', 'plasma', 'sampl', 'diabet', 'prevent', 'program', 'cox', 'model', 'use', 'determin', 'associ', 'metabolit', 'incid', 'well', 'whether', 'associ', 'differ', 'treatment', 'group', 'lifestyl', 'metformin', 'placebo', 'averag', 'year', 'follow', 'found', 'metabolit', 'associ', 'incid', 'regardless', 'treatment', 'cytosin', 'novel', 'associ', 'lowest', 'exploratori', 'baselin', 'metabolit', 'associ', 'incid', 'differ', 'across', 'treatment', 'stratif', 'baselin', 'level', 'sever', 'includ', 'specif', 'phospholipid', 'adenosin', 'monophosph', 'modifi', 'effect', 'il', 'met', 'diabet', 'find', 'highlight', 'novel', 'marker', 'diabet', 'risk', 'prevent', 'treatment', 'effect', 'individu', 'clinic', 'high', 'risk', 'motiv', 'studi', 'valid']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def stemmed_words(stemmed_words_input):\n",
    "    \"\"\"stem the words\"\"\"\n",
    "    \n",
    "    all_texts_stemmed = []\n",
    "    \n",
    "    for each_stemmed_words_input in stemmed_words_input:\n",
    "\n",
    "        stemmed = [porter.stem(word) for word in each_stemmed_words_input]\n",
    "   \n",
    "        all_texts_stemmed.append(stemmed)\n",
    "\n",
    "    return all_texts_stemmed\n",
    "\n",
    "\n",
    "stemmed_words_output_0 = stemmed_words(remove_stopwords_output_0)\n",
    "stemmed_words_output_1 = stemmed_words(remove_stopwords_output_1)\n",
    "print(stemmed_words_output_0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge doc lists into single lines.\n",
    "\n",
    "Using stemmed_words_output, the cleaned, tokenized version, join the docs back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined_docs_output_0\n",
      "novel biomark type diabet respons prevent treatment individu similar clinic risk may highlight metabol pathway import diseas profil metabolit baselin plasma sampl diabet prevent program cox model use determin associ metabolit incid well whether associ differ treatment group lifestyl metformin placebo averag year follow found metabolit associ incid regardless treatment cytosin novel associ lowest exploratori baselin metabolit associ incid differ across treatment stratif baselin level sever includ specif phospholipid adenosin monophosph modifi effect il met diabet find highlight novel marker diabet risk prevent treatment effect individu clinic high risk motiv studi valid\n",
      "\n",
      "\n",
      "joined_docs_output_0\n",
      "identifi factor mediat progress diabet nephropathi perform rna sequenc kidney biopsi sampl patient earli advanc normal kidney tissu nephrectomi set gene upregul earli downregul late shown larg includ gene retino acid pathway receptor anoth group gene downregul earli highli upregul advanc consist mostli gene associ kidney diseas relat immun respons correl egfr identifi gene pathway iron transport cell posit associ immun respons fibrosi pathway neg correl variou featur also identifi associ distinct gene ontolog deconvolut analysi dataset indic signific increas myofibroblast advanc studi thu provid potenti molecular mechan progress associ differenti gene express function structur chang observ patient earli advanc\n",
      "\n",
      "\n",
      "joined_docs_output_0\n",
      "obes take worldwid epidem yet effect agent efficaci remain design iminosugar potent improv glucos homeostasi lower excess show promot satieti activ brown adipos tissu obes demonstr mechan mediat favor action depend administr ultim stimul system evid essenti role brain receptor fail promot satieti activ bat mice lack brain well mice treat antagonist markedli amelior metabol abnorm obes rodent restor satieti activ bat central improv glucos homeostasi mechan independ central\n",
      "\n",
      "\n",
      "joined_docs_output_1\n",
      "patient metastat melanoma variabl respons combin ipilimumab object studi examin pattern respons surviv patient treat combin ipilimumab therapi explor natur acquir patient metastat melanoma receiv treatment ipi metastas measur comput reson imag lesion respons rate overal respons rate determin accord respons evalu criteria solid version patient metastas orr overal complet respons rate decreas tumor burden number metastas metastas smaller metastas without lung metastas highest lrr wherea liver metastas lowest multivari patient lung metastas superior orr ratio surviv ratio wherea liver metastas inferior orr surviv overal surviv occur patient progress diseas best overal surviv compar patient without diseas acquir resist occur respond median overal surviv rate year metastas differ anatom locat display distinct respons pattern also associ overal respons surviv combin specif site diseas may hold uniqu mechan resist allow person\n",
      "\n",
      "\n",
      "joined_docs_output_1\n",
      "elev ratio associ poor surviv patient includ receiv author sought investig nlr biomark treatment outcom patient melanoma treat patient undergo initi treatment inhibitor monotherapi stage melanoma singl center clinic nlr baselin subsequ treatment cycl time treatment failur overal surviv evalu use landmark among studi baselin nlr baselin nlr significantli associ eastern cooper oncolog group perform statu number involv metastat median month baselin nlr independ associ shorter ttf nlr increas first cycl treatment associ wors trend toward shorter ttf combin baselin nlr nlr increas identifi small cohort markedli shorten ttf elev baselin nlr increas nlr earli treatment prognost ttf patient melanoma treat inhibitor biomark wide patient treatment failur\n",
      "\n",
      "\n",
      "joined_docs_output_1\n",
      "mani parent children advanc cancer pursu cur goal cure longer pediatr studi date prospect evalu prognosi commun influenc decis make childhood author conduct prospect cohort studi pediatr cancer center enrol parent children recurr neuroblastoma condit cure rare parent survey regard likelihood primari goal qualiti regret concern last treatment medic record identifi care treatment parent recogn chanc cure ask choos singl import goal approxim chose chose longer chose qualiti parent like priorit qualiti life recogn poor prognosi approxim parent express regret recent treatment parent like experi regret child receiv higher intens medic care ratio experienc suffer limit benefit recent treatment experienc suffer symptom parent children cancer frequent make decis base unrealist new strategi effect prognosi commun\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def joined_docs(joined_docs_input):\n",
    "    \n",
    "    total_list = []\n",
    "    \n",
    "    for each_joined_docs_input in joined_docs_input:\n",
    "        \n",
    "        each_joined_docs_input = ' '.join(each_joined_docs_input)\n",
    "        \n",
    "        total_list.append(each_joined_docs_input)\n",
    "        \n",
    "    return total_list\n",
    "\n",
    "\n",
    "joined_docs_output_0 = joined_docs(stemmed_words_output_0)\n",
    "joined_docs_output_1 = joined_docs(stemmed_words_output_1)\n",
    "\n",
    "for each in joined_docs_output_0:\n",
    "    print(\"joined_docs_output_0\")\n",
    "    print(each)\n",
    "    print(\"\\n\")\n",
    "\n",
    "for each in joined_docs_output_1:\n",
    "    print(\"joined_docs_output_1\")\n",
    "    print(each)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So up to this point, I've cleaned the text, and then joined each document into it's own list element. Now, I think I need to\n",
    "  1. Keras tokenize\n",
    "  2. ?\n",
    "  \n",
    "Stop this monkey work. Just construct a dataset.\n",
    "\n",
    "70 train, 30 test\n",
    "\n",
    "train = 35 cancer, 35 diabetes\n",
    "\n",
    "test = 15 cancer, 15 diabetes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between documents is calculated as the dot product between the vectors. \n",
    "\n",
    "        book my dog\n",
    "\n",
    "doc1 = [1    2  0]\n",
    "\n",
    "doc2 = [0    0  5]\n",
    "\n",
    "doc3 = [2    4  0]\n",
    "\n",
    "doc1 * doc2 = 0. There is no overlap in vector direction, so there is 0 similarity.\n",
    "\n",
    "doc2 * doc3 = similarity exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
