{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "Garth Mortensen  \n",
    "2019.09.19\n",
    "\n",
    "## Overview\n",
    "\n",
    "I have performed text analysis using Matlab's excellent toolboxes, but never Python. This is an effort to transfer my knowledge from Matlab to Python. I'll pull some text from [Project Gutenberg](https://www.gutenberg.org/) and see what I can do with it.\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # for reading website\n",
    "import string  # for punctuation list\n",
    "import re  # regex for text cleaning\n",
    "import nltk  # natural language toolkit for __________\n",
    "# nltk.download()  # Using the GUI, download all packages, or the ones of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text\n",
    "\n",
    "Provide a URL to a book and, if the connection worked, read it into a variable.\n",
    "\n",
    "**Note**: You could use requests or another popular solution urllib2, but there are security [problems](https://www.nbu.gov.sk/skcsirt-sa-20170909-pypi/index.html) with the latter. I've opted to keep my distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The url status is: 200\n",
      "Encoding style is: ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "# Using a gutenberg book\n",
    "url = \"http://www.gutenberg.org/files/2701/2701-0.txt\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "print(\"The url status is:\", response.status_code)\n",
    "print(\"Encoding style is:\", response.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "# print a confirmation that the url leads to a succesful connection.\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: %s' % (exc))\n",
    "\n",
    "if response.ok:  # != 4xx or 5xx\n",
    "    print(\"Connection successful\")\n",
    "\n",
    "# name change\n",
    "text = response.text\n",
    "del response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the content makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in story: 1276201\n"
     ]
    }
   ],
   "source": [
    "print(\"Total characters in story:\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the text. Chapter 1 starts at character 29455, so I'll simply explicitly preview this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Loomings.\n",
      "\n",
      "Call me Ishmael. Some years agoânever mind how long preciselyâhaving\n",
      "little or no money in my purse, and nothing particular to interest me\n",
      "on shore, I thought I would sail about a little and see the watery part\n",
      "of the world. I\n"
     ]
    }
   ],
   "source": [
    "# remove everything before chapter 1\n",
    "text = text[29455:]\n",
    "\n",
    "# display first 250 characters in book.\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some cleaning is required.\n",
    "\n",
    "## Text Cleaning with Python\n",
    "\n",
    "Let's split by whitespace and the following punctuation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Loomings', 'Call', 'me', 'Ishmael', 'Some', 'years', 'agoâ\\x80\\x94never', 'mind', 'how', 'long', 'preciselyâ\\x80\\x94having', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', 'I', 'thought', 'I', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', 'It', 'is', 'a', 'way', 'I', 'have', 'of', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation', 'Whenever', 'I', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', 'whenever', 'it', 'is', 'a', 'damp', 'drizzly', 'November', 'in', 'my', 'soul', 'whenever', 'I', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses', 'and', 'bringing', 'up', 'the', 'rear', 'of', 'every', 'funeral', 'I', 'meet', 'and', 'especially', 'whenever', 'my']\n"
     ]
    }
   ],
   "source": [
    "# split by white spaces\n",
    "words = text.split()\n",
    "\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in words]\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some unprintable characters. This seems to be because the text document is encoded in [ISO-8859](https://www.gutenberg.org/wiki/Gutenberg:File_Formats_FAQ#ISO-8859.2FISO-Latin_.28Character_Sets.29), instead of utf-8. These characters will simply be stripped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 211388\n",
      "['1.', 'Loomings.', 'Call', 'me', 'Ishmael.', 'Some', 'years', 'agonever', 'mind', 'how', 'long', 'preciselyhaving', 'little', 'or', 'no', 'money', 'in', 'my', 'purse,', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore,', 'I', 'thought', 'I', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world.', 'It', 'is', 'a', 'way', 'I', 'have', 'of', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation.', 'Whenever', 'I', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth;', 'whenever', 'it', 'is', 'a', 'damp,', 'drizzly', 'November', 'in', 'my', 'soul;', 'whenever', 'I', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses,', 'and', 'bringing', 'up', 'the', 'rear', 'of', 'every', 'funeral', 'I', 'meet;', 'and', 'especially', 'whenever', 'my']\n"
     ]
    }
   ],
   "source": [
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "result = [re_print.sub('', w) for w in words]\n",
    "\n",
    "print(\"List length:\", len(result))\n",
    "print(result[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There remains issues, such as precisely-having being converted to preciselyhaving. The purpose of this exercise is not to perfect the text-cleaning process, but explore text-analysis. I will move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning with nltk\n",
    "\n",
    "### Tokenize\n",
    "According to [MathWorks](https://www.mathworks.com/help/textanalytics/ref/tokenizeddocument.html),\n",
    "> A **tokenized** document is a document represented as a collection of words (also known as tokens) which is used for text analysis.\n",
    "\n",
    "First, let's tokenize this into sentances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 8712\n",
      "[' 1.', 'Loomings.', 'Call me Ishmael.', 'Some years agoâ\\x80\\x94never mind how long preciselyâ\\x80\\x94having\\r\\nlittle or no money in my purse, and nothing particular to interest me\\r\\non shore, I thought I would sail about a little and see the watery part\\r\\nof the world.', 'It is a way I have of driving off the spleen and\\r\\nregulating the circulation.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "# split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"List length:\", len(sentences))\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of \\r\\n (carriage return and new line) indicates the artificial word-wrapping was preserved.\n",
    "\n",
    "Next, let's tokenize into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 246386\n",
      "['1', '.', 'Loomings', '.', 'Call', 'me', 'Ishmael', '.', 'Some', 'years', 'agoâ\\x80\\x94never', 'mind', 'how', 'long', 'preciselyâ\\x80\\x94having', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', ',', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', ',', 'I', 'thought', 'I', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', '.', 'It', 'is', 'a', 'way', 'I', 'have', 'of', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation', '.', 'Whenever', 'I', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', ';', 'whenever', 'it', 'is', 'a', 'damp', ',', 'drizzly', 'November', 'in', 'my', 'soul', ';', 'whenever', 'I', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses', ',', 'and', 'bringing', 'up']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(\"List length:\", len(tokens))\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have punctuation included as tokens. Let's remove it by filtering out anything that doesn't consist of alphabetical characters.\n",
    "\n",
    "Let's start using [list comprehensions](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python) to process the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 204265\n",
      "['Loomings', 'Call', 'me', 'Ishmael', 'Some', 'years', 'mind', 'how', 'long', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', 'I', 'thought', 'I', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', 'It', 'is', 'a', 'way', 'I', 'have', 'of', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation', 'Whenever', 'I', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', 'whenever', 'it', 'is', 'a', 'damp', 'drizzly', 'November', 'in', 'my', 'soul', 'whenever', 'I', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses', 'and', 'bringing', 'up', 'the', 'rear', 'of', 'every', 'funeral', 'I', 'meet', 'and', 'especially', 'whenever', 'my', 'hypos', 'get', 'such']\n"
     ]
    }
   ],
   "source": [
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "print(\"List length:\", len(words))\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gone  \n",
    "from: _\"Some years agoâ\\x80\\x94never mind\"_  \n",
    "to: _\"Some years mind\"_  \n",
    "Which, for this exercise, is an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 204265\n",
      "['loomings', 'call', 'me', 'ishmael', 'some', 'years', 'mind', 'how', 'long', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', 'i', 'thought', 'i', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', 'it', 'is', 'a', 'way', 'i', 'have', 'of', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation', 'whenever', 'i', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', 'whenever', 'it', 'is', 'a', 'damp', 'drizzly', 'november', 'in', 'my', 'soul', 'whenever', 'i', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses', 'and', 'bringing', 'up', 'the', 'rear', 'of', 'every', 'funeral', 'i', 'meet', 'and', 'especially', 'whenever', 'my', 'hypos', 'get', 'such']\n"
     ]
    }
   ],
   "source": [
    "words = [w.lower() for w in words]\n",
    "print(\"List length:\", len(words))\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Now let's chop out all the stopwords. **Stopwords** are generally the highest frequency words in a language. You can more or less find English language stopwords using this [technique](https://youtu.be/zth-Awh2xWk). Just determine if your stopwords will be the top 100 or 1,000 words. Exploring the stopwords provided by nltk, n varies widely.\n",
    "\n",
    "I found the supported languages here:\n",
    "> C:\\Users\\grm\\AppData\\Roaming\\nltk_data\\corpora\\stopwords\n",
    "\n",
    "and used cmd to pipe the list to a .txt file:\n",
    "> dir > languageList.txt\n",
    "\n",
    "to write the results to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stopwords by language\n",
      "arabic : \t 248\n",
      "azerbaijani : \t 165\n",
      "danish : \t 94\n",
      "dutch : \t 101\n",
      "english : \t 179\n",
      "finnish : \t 235\n",
      "french : \t 157\n",
      "german : \t 232\n",
      "greek : \t 265\n",
      "hungarian : \t 199\n",
      "indonesian : \t 758\n",
      "italian : \t 279\n",
      "kazakh : \t 324\n",
      "nepali : \t 255\n",
      "norwegian : \t 176\n",
      "portuguese : \t 204\n",
      "romanian : \t 356\n",
      "russian : \t 151\n",
      "slovene : \t 1784\n",
      "spanish : \t 313\n",
      "swedish : \t 114\n",
      "tajik : \t 163\n",
      "turkish : \t 53\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "language = [\"arabic\", \"azerbaijani\", \"danish\", \"dutch\", \"english\", \"finnish\", \"french\", \"german\", \"greek\", \"hungarian\", \"indonesian\", \"italian\", \"kazakh\", \"nepali\", \"norwegian\", \"portuguese\", \"romanian\", \"russian\", \"slovene\", \"spanish\", \"swedish\", \"tajik\", \"turkish\"]\n",
    "\n",
    "print(\"Total stopwords by language\")\n",
    "for lan in language:\n",
    "    stop_words = stopwords.words(lan)\n",
    "    print(lan, \": \\t\", len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity, let's see the stopwords in interesting languages. I wonder if we can see similarities in Turkish languages (I think this covers Central Asia? And non-Indo-European languages in Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 stopwords in turkish:\n",
      "['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz']\n",
      "\n",
      "Top 10 stopwords in azerbaijani:\n",
      "['a', 'ad', 'altı', 'altmış', 'amma', 'arasında', 'artıq', 'ay', 'az', 'bax']\n",
      "\n",
      "Top 10 stopwords in kazakh:\n",
      "['ах', 'ох', 'эх', 'ай', 'эй', 'ой', 'тағы', 'тағыда', 'әрине', 'жоқ']\n",
      "\n",
      "Top 10 stopwords in finnish:\n",
      "['olla', 'olen', 'olet', 'on', 'olemme', 'olette', 'ovat', 'ole', 'oli', 'olisi']\n",
      "\n",
      "Top 10 stopwords in hungarian:\n",
      "['a', 'ahogy', 'ahol', 'aki', 'akik', 'akkor', 'alatt', 'által', 'általában', 'amely']\n"
     ]
    }
   ],
   "source": [
    "languages = [\"turkish\", \"azerbaijani\", \"kazakh\", \"finnish\", \"hungarian\"]\n",
    "\n",
    "for lan in languages:\n",
    "    stop_words = stopwords.words(lan)\n",
    "    print(\"\\nTop 10 stopwords in \" + lan + \":\")\n",
    "    print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, back on track. Let's see English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total English stopwords: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(\"\\ntotal English stopwords:\", len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take out the stopwords using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 102487\n",
      "['loomings', 'call', 'ishmael', 'years', 'mind', 'long', 'little', 'money', 'purse', 'nothing', 'particular', 'interest', 'shore', 'thought', 'would', 'sail', 'little', 'see', 'watery', 'part', 'world', 'way', 'driving', 'spleen', 'regulating', 'circulation', 'whenever', 'find', 'growing', 'grim', 'mouth', 'whenever', 'damp', 'drizzly', 'november', 'soul', 'whenever', 'find', 'involuntarily', 'pausing', 'coffin', 'warehouses', 'bringing', 'rear', 'every', 'funeral', 'meet', 'especially', 'whenever', 'hypos', 'get', 'upper', 'hand', 'requires', 'strong', 'moral', 'principle', 'prevent', 'deliberately', 'stepping', 'street', 'methodically', 'knocking', 'hats', 'account', 'high', 'time', 'get', 'sea', 'soon', 'substitute', 'pistol', 'ball', 'philosophical', 'flourish', 'cato', 'throws', 'upon', 'sword', 'quietly', 'take', 'ship', 'nothing', 'surprising', 'knew', 'almost', 'men', 'degree', 'time', 'cherish', 'nearly', 'feelings', 'towards', 'ocean', 'insular', 'city', 'manhattoes', 'belted', 'round', 'wharves']\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if not w in stop_words]\n",
    "print(\"List length:\", len(words))\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gone  \n",
    "from:  _\"Call me Ishmael\"_  \n",
    "to: _\"call ishmael\"_  \n",
    "which is an improvement.\n",
    "\n",
    "### Stemming\n",
    "\n",
    "According to [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) \n",
    ">**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "Porter's stemmer is the most well known for stemming. Time and place = Cambridge (Oxford), 1980. There are stemmers for other languages that follow other rules.\n",
    "\n",
    "The goal is to reduce the total vocabulary space by simplifying words into their stems, chopping off the suffix.\n",
    "\n",
    "_cat_ <- car, cats, cat's, cats'\n",
    "\n",
    "Let's get stemmy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 102487\n",
      "['loom', 'call', 'ishmael', 'year', 'mind', 'long', 'littl', 'money', 'purs', 'noth', 'particular', 'interest', 'shore', 'thought', 'would', 'sail', 'littl', 'see', 'wateri', 'part', 'world', 'way', 'drive', 'spleen', 'regul', 'circul', 'whenev', 'find', 'grow', 'grim', 'mouth', 'whenev', 'damp', 'drizzli', 'novemb', 'soul', 'whenev', 'find', 'involuntarili', 'paus', 'coffin', 'warehous', 'bring', 'rear', 'everi', 'funer', 'meet', 'especi', 'whenev', 'hypo', 'get', 'upper', 'hand', 'requir', 'strong', 'moral', 'principl', 'prevent', 'deliber', 'step', 'street', 'method', 'knock', 'hat', 'account', 'high', 'time', 'get', 'sea', 'soon', 'substitut', 'pistol', 'ball', 'philosoph', 'flourish', 'cato', 'throw', 'upon', 'sword', 'quietli', 'take', 'ship', 'noth', 'surpris', 'knew', 'almost', 'men', 'degre', 'time', 'cherish', 'nearli', 'feel', 'toward', 'ocean', 'insular', 'citi', 'manhatto', 'belt', 'round', 'wharv']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# stem the words\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "print(\"List length:\", len(stemmed))\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this has produced misspelled and sometimes difficult to recognize words. Thankfully, text analysis does not require perfectly cleaned text files. We can move on with this working document."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
