{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data\n",
    "Databricks + Spark  \n",
    "2020.03.06\n",
    "\n",
    "Render markdown with [%md](https://forums.databricks.com/answers/9506/view.html)\n",
    "\n",
    "## Tasks\n",
    "\n",
    "I register at Databricks and use a Community Edition instance. It's free, [here](https://community.cloud.databricks.com/).\n",
    "\n",
    "Then, I create a notebook, attach a cluster to it, add a file to the cluster, then run some Actions, Transformations, and Functions on it.\n",
    "\n",
    "**Note - I believe that after 2 hours of idling, your cluster is spun-down. You can clone it and reattach a notebook to it though.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Apache Spark\n",
    "\n",
    "[Apache Spark](https://en.wikipedia.org/wiki/Apache_Spark) created Databricks. \n",
    "\n",
    "> Apache Spark is a cluster computing platform designed to be *fast and general purpose*. \n",
    "\n",
    "> 1. **Speed**, Spark extends the popular MapReduce model to efficiently support more types of computations, including **interactive queries and stream processing**. Speed is important in processing large datasets, as it means the difference between exploring data interactively and waiting minutes or hours. One of the main features Spark offers for speed is the ability to run **computations in memory**...\n",
    "\n",
    "> 2. **Generality**, Spark is designed to cover a wide range of workloads that previously required separate distributed systems, including **batch applications, iterative algorithms, interactive queries, and streaming. By supporting these workloads in the same engine, Spark makes it easy and inexpensive to combine different processing types, which is often necessary in production data analysis pipelines. In addition, it reduces the management burden of maintaining separate tools.**\n",
    "\n",
    "> 3. **Highly accessible**, offering **simple APIs in Python, Java, Scala, and SQL, and rich built-in libraries**. It also integrates closely with other Big Data tools. In particular, Spark can run in Hadoop clusters and access any Hadoop data source, including Cassandra.\n",
    "\n",
    "[Learning Spark](https://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624). Page 1. Note - this book was written by 4 Databricks employees/founders.\n",
    "\n",
    "### Databricks\n",
    "\n",
    "[Databricks](https://en.wikipedia.org/wiki/Databricks) is a company founded by the original Apache Spark creators, and grew out of the AMPLab project at Berkeley which was involved in creating Spark. \n",
    "\n",
    "> Databricks develops a **web-based platform for working with Spark**, that provides **automated cluster management and IPython-style notebooks**. In addition to building the Databricks platform, the company is co-organizing **massive open online courses**.\n",
    "\n",
    "They have a huuuuuuuge trove of videos on [YT](https://www.youtube.com/channel/UC3q8O3Bh2Le8Rj1-Q-_UUbA/videos)\n",
    "\n",
    "#### Spark Basics\n",
    "\n",
    "Every Spark application consists of a **driver program that runs the user’s main function and executes various parallel operations on a cluster**. The main abstraction Spark provides is a resilient distributed dataset, **RDD, which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel**. \n",
    "\n",
    "**RDDs are created by starting with a file in the Hadoop file system, and transforming it**. Users may also ask Spark to **persist an RDD in memory**, allowing it to be reused efficiently across parallel operations. Finally, **RDDs automatically recover from node failures**.\n",
    "\n",
    "A second abstraction in Spark is **shared variables that can be used in parallel operations**. By default, when Spark runs a function in parallel as a set of tasks on different nodes, **it ships a copy of each variable used in the function to each task**. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: **broadcast variables, which can be used to cache a value in memory on all nodes**, and **accumulators, which are variables that are only “added” to, such as counters and sums**.\n",
    "\n",
    "[Source](https://spark.apache.org/docs/latest/rdd-programming-guide.html#overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDDs\n",
    "\n",
    "Resilient distributed dataset - a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs\n",
    "\n",
    "1 - Parallelizing an existing collection in your driver program. = Parallelized collections are created on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel.\n",
    "\n",
    "```spark\n",
    "data = [1, 2, 3, 4, 5]  \n",
    "distData = sc.parallelize(data)  \n",
    "```\n",
    "\n",
    "2 - Referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, etc. This method takes a file URI and reads it as a collection of lines.\n",
    "\n",
    "```spark\n",
    "distFile = sc.textFile(\"data.txt\")\n",
    "```\n",
    "\n",
    "[Source](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)\n",
    "\n",
    "#### RDD Operations\n",
    "\n",
    "RDDs support two types of operations\n",
    "1. Transformations - Create a new dataset from an existing one\n",
    "2. Actions - Return a value to the driver program after running a computation on the dataset.\n",
    "\n",
    "For example, **map is a transformation** that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, **reduce is an action** that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
    "\n",
    "**Transformations are lazy, in that they do not compute their results right away**. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
    "\n",
    "By default, **each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory** using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - [RDD Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n",
    "\n",
    "Actions return a value to the driver program after running a computation on the dataset.\n",
    "\n",
    "### Count\n",
    "\n",
    "Return the number of elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[3]: 5</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an RDD \n",
    "data_variable = [9, 10, 5, 1, 2]\n",
    "rdd = sc.parallelize(data_variable)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce(func)\n",
    "\n",
    "Aggregate the elements of the dataset using a function func (which takes two arguments and returns one).  \n",
    "\n",
    "The function should be commutative and associative so that it can be computed correctly in parallel. Example of commutative and associative function:  \n",
    "a + b = b + a and a + (b + c) = (a + b) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[8]: 27</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an RDD\n",
    "data_variable = [9, 10, 5, 1, 2]\n",
    "rdd = sc.parallelize(data_variable)\n",
    "\n",
    "# test reduce(func) API \n",
    "rdd.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[9]: 10</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an RDD\n",
    "data_variable = [9, 10, 5, 1, 2]\n",
    "rdd = sc.parallelize(data_variable)\n",
    "\n",
    "# test reduce(func) API \n",
    "rdd.reduce(lambda a, b: a if a > b else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect()\n",
    "\n",
    "Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[10]: [9, 10, 5, 1, 2]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an RDD\n",
    "data_variable = [9, 10, 5, 1, 2]\n",
    "rdd = sc.parallelize(data_variable)\n",
    "\n",
    "# test collect() API \n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take(n)\n",
    "Return an array with the first n elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[11]: [9, 10]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an RDD\n",
    "data_variable = [9, 10, 5, 1, 2]\n",
    "rdd = sc.parallelize(data_variable)\n",
    "\n",
    "# test collect() API\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile(path)\n",
    "\n",
    "Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an RDD\n",
    "data = [9, 10, 5, 1, 2]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# test saveAsTextFile() API\n",
    "rdd.saveAsTextFile(\"/tmp/file1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - [RDD Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)\n",
    "\n",
    "Transformations create a new dataset from an existing one.\n",
    "\n",
    "### map(func)\n",
    "Return a new distributed dataset formed by passing each element of the source through a function func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# apply map(func) transformation to the RDD\n",
    "rdd1 = rdd.map(lambda x: x * 5 + 1)\n",
    "\n",
    "# show results of the new rdd\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatmap(func)\n",
    "\n",
    "Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD using sample data\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# apply map(func) transformation to the RDD\n",
    "rdd2 = rdd.flatMap(lambda x: [x, x * 3])\n",
    "\n",
    "# show results of a new rdd\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "Return a new dataset formed by selecting those elements of the source on which func returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD using sample data\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# apply filter(func) transformation to the RDD\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-value pairs\n",
    "\n",
    "A Key/Value RDD is an RDD whose elements comprise a pair of values – key and value. It should be in a tuple format such as (1,2) and then you apply key-value pair operations . For example: join(), groupByKey(), or reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the textFile RDD to read the README.md file\n",
    "# Note: this is lazy\n",
    "textFile = sc.textFile(\"databricks-datasets/samples/docs/README.md\")\n",
    "\n",
    "# split each line of readme file to words first, and then make a tuple of (word, 1)\n",
    "textFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "\n",
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the textFile RDD to read the README.md file\n",
    "textFile = sc.textFile(\"databricks-datasets/samples/docs/README.md\")\n",
    "\n",
    "# split each line of readme file to words first, and then make a tuple of word, 1\n",
    "rdd_key = textFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n",
    "\n",
    "# reduce by key\n",
    "\n",
    "rdd_key.reduceByKey(lambda x, y: x + y).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union\n",
    "\n",
    "Return a new dataset that contains the union of the elements in the source dataset and the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some rdds\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "\n",
    "# combine these rdds with a union\n",
    "rdd1.union(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey()\n",
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the textFile RDD to read the README.md file\n",
    "textFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n",
    "\n",
    "# split each line of the readme file to words, and then make a tuple of word, 1.\n",
    "rdd_key = textFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n",
    "\n",
    "# group by key\n",
    "rdd_gp = rdd_key.groupByKey()\n",
    "\n",
    "for (key, value) in rdd_gp.take(5):\n",
    "  print(key, sum(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join\n",
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two RRDs\n",
    "rdd1 = sc.parallelize([('rock', 1), ('paper', 2), ('scissor', 1), ('hammer', 3)])\n",
    "rdd2 = sc.parallelize([('hammer', 2), ('paper', 3), ('water', 1), ('fire', 3)])\n",
    "\n",
    "# perform left outer join\n",
    "rdd2.leftOuterJoin(rdd1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats()\n",
    "\n",
    "Return the count, mean, standard deviation, max and min of the RDDs' elements in one operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats transformation to get mean, count, std dev\n",
    "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9]).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample(withReplacement, fraction, seed=None)\n",
    "\n",
    "Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.  \n",
    "Parameters:\t\n",
    "withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "fraction – expected size of the sample as a fraction of this RDD’s size without\n",
    "seed – seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the textFile RDD to read the README.md file\n",
    "textFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n",
    "\n",
    "# split each line of the readme file to words first, then make a truple of the word.\n",
    "rdd_key = textFile.flatMap(lambda x: x.split(' '))\n",
    "rdd_key.sample(False, 0.02, 3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions \n",
    "Create a function and use it for a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that tells if the line is small or large.\n",
    "\n",
    "def strLenType(input):\n",
    "  if len(input) < 15:\n",
    "    return \"Small\"\n",
    "  else:\n",
    "    return \"Large\"\n",
    "\n",
    "# Setup the textFile RDD to read the README.md file\n",
    "textFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n",
    "\n",
    "# split each line of the readme to words, then tuple-ize them\n",
    "textFile.map(lambda x: strLenType(x)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Put several things together\n",
    "\n",
    "### map(func)\n",
    "Return a new distributed dataset formed by passing each element of the source through a function func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file = \"/FileStore/tables/cities.txt\"\n",
    "cities = sc.textFile(file)\n",
    "cities.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the header row, create a key value pair and reduce by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a key value pair of state, and 1 for each record\n",
    "header = cities.first()\n",
    "\n",
    "cities1 = cities.filter(lambda row: row != header)\n",
    "cities1.take(1)\n",
    "cities2 = cities1.map(lambda row: row.split(\",\"))\n",
    "cities_key = cities2.map(lambda row: (row[9], 1))\n",
    "cities_key.reduceByKey(lambda x, y: x + y).take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Spark_RDDs",
  "notebookId": 2659105190132887
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
