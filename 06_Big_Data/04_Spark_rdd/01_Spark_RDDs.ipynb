{"cells":[{"cell_type":"markdown","source":["# Big Data\nDatabricks + Spark  \n2020.03.06\n\nRender markdown with %md\n\n## Tasks\n\nRegister at Databricks and use a Community Edition instance. It's free, [here](https://community.cloud.databricks.com/).\n\nCreate a notebook, attach a cluster to it, add a file to the cluster, then run some Actions, Transformations, and Functions on it.\n\n**Note - I believe that after 2 hours of idling, your cluster is spun-down. You can clone it and reattach a notebook to it though.**"],"metadata":{}},{"cell_type":"markdown","source":["## Overview\n\n### Apache Spark\n\n[Apache Spark](https://en.wikipedia.org/wiki/Apache_Spark) created Databricks. \n\n> Apache Spark is a cluster computing platform designed to be *fast and general purpose*. \n\n> 1. **Speed**, Spark extends the popular MapReduce model to efficiently support more types of computations, including **interactive queries and stream processing**. Speed is important in processing large datasets, as it means the difference between exploring data interactively and waiting minutes or hours. One of the main features Spark offers for speed is the ability to run **computations in memory**...\n\n> 2. **Generality**, Spark is designed to cover a wide range of workloads that previously required separate distributed systems, including **batch applications, iterative algorithms, interactive queries, and streaming. By supporting these workloads in the same engine, Spark makes it easy and inexpensive to combine different processing types, which is often necessary in production data analysis pipelines. In addition, it reduces the management burden of maintaining separate tools.**\n\n> 3. **Highly accessible**, offering **simple APIs in Python, Java, Scala, and SQL, and rich built-in libraries**. It also integrates closely with other Big Data tools. In particular, Spark can run in Hadoop clusters and access any Hadoop data source, including Cassandra.\n\n[Learning Spark](https://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624). Page 1. Note - this book was written by 4 Databricks employees/founders.\n\n### Databricks\n\n[Databricks](https://en.wikipedia.org/wiki/Databricks) is a company founded by the original Apache Spark creators, and grew out of the AMPLab project at Berkeley which was involved in creating Spark. \n\n> Databricks develops a **web-based platform for working with Spark**, that provides **automated cluster management and IPython-style notebooks**. In addition to building the Databricks platform, the company is co-organizing **massive open online courses**.\n\nThey have a huuuuuuuge trove of videos on [YT](https://www.youtube.com/channel/UC3q8O3Bh2Le8Rj1-Q-_UUbA/videos)\n\n#### Spark Basics\n\nEvery Spark application consists of a **driver program that runs the user’s main function and executes various parallel operations on a cluster**. The main abstraction Spark provides is a resilient distributed dataset, **RDD, which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel**. \n\n**RDDs are created by starting with a file in the Hadoop file system, and transforming it**. Users may also ask Spark to **persist an RDD in memory**, allowing it to be reused efficiently across parallel operations. Finally, **RDDs automatically recover from node failures**.\n\nA second abstraction in Spark is **shared variables that can be used in parallel operations**. By default, when Spark runs a function in parallel as a set of tasks on different nodes, **it ships a copy of each variable used in the function to each task**. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: **broadcast variables, which can be used to cache a value in memory on all nodes**, and **accumulators, which are variables that are only “added” to, such as counters and sums**.\n\n[Source](https://spark.apache.org/docs/latest/rdd-programming-guide.html#overview)"],"metadata":{}},{"cell_type":"markdown","source":["#### RDDs\n\nResilient distributed dataset - a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs\n\n1 - Parallelizing an existing collection in your driver program. = Parallelized collections are created on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel.\n\n```spark\ndata = [1, 2, 3, 4, 5]  \ndistData = sc.parallelize(data)  \n```\n\n2 - Referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, etc. This method takes a file URI and reads it as a collection of lines.\n\n```spark\ndistFile = sc.textFile(\"data.txt\")\n```\n\n[Source](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)\n\n#### RDD Operations\n\nRDDs support two types of operations\n1. Transformations - Create a new dataset from an existing one\n2. Actions - Return a value to the driver program after running a computation on the dataset.\n\nFor example, **map is a transformation** that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, **reduce is an action** that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n\n**Transformations are lazy, in that they do not compute their results right away**. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n\nBy default, **each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory** using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it."],"metadata":{}},{"cell_type":"markdown","source":["## Part 1 - [RDD Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n\nActions return a value to the driver program after running a computation on the dataset.\n\n### Count\n\nReturn the number of elements in the dataset."],"metadata":{}},{"cell_type":"code","source":["# Create an RDD \ndata_variable = [9, 10, 5, 1, 2]\nrdd = sc.parallelize(data_variable)\nrdd.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: 5</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Reduce(func)\n\nAggregate the elements of the dataset using a function func (which takes two arguments and returns one).  \n\nThe function should be commutative and associative so that it can be computed correctly in parallel. Example of commutative and associative function:  \na + b = b + a and a + (b + c) = (a + b) + c"],"metadata":{}},{"cell_type":"code","source":["# Create an RDD\ndata_variable = [9, 10, 5, 1, 2]\nrdd = sc.parallelize(data_variable)\n\n# test reduce(func) API \nrdd.reduce(lambda a, b: a + b)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: 27</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Create an RDD\ndata_variable = [9, 10, 5, 1, 2]\nrdd = sc.parallelize(data_variable)\n\n# test reduce(func) API \nrdd.reduce(lambda a, b: a if a > b else b)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: 10</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["### collect()\n\nReturn all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data."],"metadata":{}},{"cell_type":"code","source":["# Create an RDD\ndata_variable = [9, 10, 5, 1, 2]\nrdd = sc.parallelize(data_variable)\n\n# test collect() API \nrdd.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [9, 10, 5, 1, 2]</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["### Take(n)\nReturn an array with the first n elements of the dataset."],"metadata":{}},{"cell_type":"code","source":["# Create an RDD\ndata_variable = [9, 10, 5, 1, 2]\nrdd = sc.parallelize(data_variable)\n\n# test collect() API\nrdd.take(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [9, 10]</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### saveAsTextFile(path)\n\nWrite the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file."],"metadata":{}},{"cell_type":"code","source":["# Create an RDD\ndata = [9, 10, 5, 1, 2]\nrdd = sc.parallelize(data)\n\n# test saveAsTextFile() API\n# rdd.saveAsTextFile(\"/tmp/file1.txt\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1614761948250661&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-red-fg\"># test saveAsTextFile() API</span>\n<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\"> </span>rdd<span class=\"ansi-blue-fg\">.</span>saveAsTextFile<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/tmp/file1.txt&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">saveAsTextFile</span><span class=\"ansi-blue-fg\">(self, path, compressionCodecClass)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1611</span>             self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>saveAsTextFileImpl<span class=\"ansi-blue-fg\">(</span>keyed<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> compressionCodecClass<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1612</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1613</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>saveAsTextFileImpl<span class=\"ansi-blue-fg\">(</span>keyed<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1614</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1615</span>     <span class=\"ansi-red-fg\"># Pair functions</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/tmp/file1.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1562)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1541)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1541)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1541)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:559)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$._saveAsTextFile(PythonRDD.scala:858)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsTextFileImpl(PythonRDD.scala:824)\n\tat org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Part 2 - [RDD Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)\n\nTransformations create a new dataset from an existing one.\n\n### map(func)\nReturn a new distributed dataset formed by passing each element of the source through a function func."],"metadata":{}},{"cell_type":"code","source":["# Create an RDD\nrdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# apply map(func) transformation to the RDD\nrdd1 = rdd.map(lambda x: x * 3)\n\n# show results of the new rdd\nrdd1.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[28]: [3, 6, 9, 12, 15]</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### flatmap(func)\n\nSimilar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."],"metadata":{}},{"cell_type":"code","source":["# Create RDD using sample data\nrdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# apply map(func) transformation to the RDD\nrdd2 = rdd.flatMap(lambda x: [x, x * 3])\n\n# show results of a new rdd\nrdd2.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: [1, 3, 2, 6, 3, 9, 4, 12, 5, 15]</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["### Filter\nReturn a new dataset formed by selecting those elements of the source on which func returns true."],"metadata":{}},{"cell_type":"code","source":["# Create RDD using sample data\nrdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# apply filter(func) transformation to the RDD\nrdd.filter(lambda x: x % 2 == 0).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: [2, 4]</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["### Key-value pairs\n\nA Key/Value RDD is an RDD whose elements comprise a pair of values – key and value. It should be in a tuple format such as (1,2) and then you apply key-value pair operations . For example: join(), groupByKey(), or reduceByKey()"],"metadata":{}},{"cell_type":"code","source":["# Setup the textFile RDD to read the README.md file\n# Note: this is lazy\ntextFile = sc.textFile(\"databricks-datasets/samples/docs/README.md\")\n\n# split each line of readme file to words first, and then make a tuple of (word, 1)\ntextFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: [(&#39;Welcome&#39;, 1)]</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["### reduceByKey\n\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."],"metadata":{}},{"cell_type":"code","source":["# Setup the textFile RDD to read the README.md file\ntextFile = sc.textFile(\"databricks-datasets/samples/docs/README.md\")\n\n# split each line of readme file to words first, and then make a tuple of word, 1\nrdd_key = textFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n\n# reduce by key\n\nrdd_key.reduceByKey(lambda x, y: x + y).take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [(&#39;Welcome&#39;, 1), (&#39;Spark&#39;, 9), (&#39;documentation!&#39;, 1)]</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["### union\n\nReturn a new dataset that contains the union of the elements in the source dataset and the argument."],"metadata":{}},{"cell_type":"code","source":["# create some rdds\nrdd1 = sc.parallelize([1, 2, 3, 4, 5])\nrdd2 = rdd1.map(lambda x: x * 2)\n\n# combine these rdds with a union\nrdd1.union(rdd2).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [1, 2, 3, 4, 5, 2, 4, 6, 8, 10]</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["### groupByKey()\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance."],"metadata":{}},{"cell_type":"code","source":["# Setup the textFile RDD to read the README.md file\ntextFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n\n# split each line of the readme file to words, and then make a tuple of word, 1.\nrdd_key = textFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n\n# group by key\nrdd_gp = rdd_key.groupByKey()\n\nfor (key, value) in rdd_gp.take(5):\n  print(key, sum(value))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Welcome 1\nSpark 9\ndocumentation! 1\n 71\nreadme 1\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["### join\nWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin."],"metadata":{}},{"cell_type":"code","source":["# create two RRDs\nrdd1 = sc.parallelize([('rock', 1), ('paper', 2), ('scissor', 1), ('hammer', 3)])\nrdd2 = sc.parallelize([('hammer', 2), ('paper', 3), ('water', 1), ('fire', 3)])\n\n# perform left outer join\nrdd2.leftOuterJoin(rdd1).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[16]: [(&#39;paper&#39;, (3, 2)),\n (&#39;hammer&#39;, (2, 3)),\n (&#39;water&#39;, (1, None)),\n (&#39;fire&#39;, (3, None))]</div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["### stats()\n\nReturn the count, mean, standard deviation, max and min of the RDDs' elements in one operation."],"metadata":{}},{"cell_type":"code","source":["# stats transformation to get mean, count, std dev\nsc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9]).stats()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[17]: (count: 9, mean: 5.0, stdev: 2.581988897471611, max: 9.0, min: 1.0)</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["### sample(withReplacement, fraction, seed=None)\n\nSample a fraction fraction of the data, with or without replacement, using a given random number generator seed.  \nParameters:\t\nwithReplacement – can elements be sampled multiple times (replaced when sampled out)\nfraction – expected size of the sample as a fraction of this RDD’s size without\nseed – seed for the random number generator"],"metadata":{}},{"cell_type":"code","source":["# Setup the textFile RDD to read the README.md file\ntextFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n\n# split each line of the readme file to words first, then make a truple of the word.\nrdd_key = textFile.flatMap(lambda x: x.split(' '))\nrdd_key.sample(False, 0.02, 3).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: [&#39;in&#39;,\n &#39;the&#39;,\n &#39;the&#39;,\n &#39;the&#39;,\n &#39;which&#39;,\n &#39;while)&#39;,\n &#39;&#39;,\n &#39;site&#39;,\n &#39;Documentation&#39;,\n &#39;SPARK_PROJECT_ROOT/R/create-docs.sh.&#39;,\n &#39;various&#39;]</div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["### Functions \nCreate a function and use it for a transformation."],"metadata":{}},{"cell_type":"code","source":["# create a function that tells if the line is small or large.\n\ndef strLenType(input):\n  if len(input) < 15:\n    return \"Small\"\n  else:\n    return \"Large\"\n\n# Setup the textFile RDD to read the README.md file\ntextFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n\n# split each line of the readme to words, then tuple-ize them\ntextFile.map(lambda x: strLenType(x)).take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[19]: [&#39;Large&#39;, &#39;Small&#39;, &#39;Large&#39;, &#39;Large&#39;, &#39;Large&#39;]</div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["## Part 3 - Put several things together\n\n### map(func)\nReturn a new distributed dataset formed by passing each element of the source through a function func."],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile = \"/FileStore/tables/cities.txt\"\ncities = sc.textFile(file)\ncities.take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: [&#39;LatD, LatM, LatS, NS, LonD, LonM, LonS, EW, City, State&#39;,\n &#39;41,5,59, N,80,39,0, W, Youngstown, OH&#39;,\n &#39;42,52,48, N,97,23,23, W, Yankton, SD&#39;]</div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["Remove the header row, create a key value pair and reduce by key."],"metadata":{}},{"cell_type":"code","source":["# create a key value pair of state, and 1 for each record\nheader = cities.first()\n\ncities1 = cities.filter(lambda row: row != header)\ncities1.take(4)  # effectively, print 4 rows\ncities2 = cities1.map(lambda row: row.split(\",\"))\ncities_key = cities2.map(lambda row: (row[9], 1))\ncities_key.reduceByKey(lambda x, y: x + y).take(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[26]: [(&#39; SD&#39;, 3),\n (&#39; WI&#39;, 4),\n (&#39; NC&#39;, 3),\n (&#39; DE&#39;, 1),\n (&#39; KS&#39;, 3),\n (&#39; FL&#39;, 5),\n (&#39; IL&#39;, 3),\n (&#39; BC&#39;, 1),\n (&#39; OK&#39;, 2),\n (&#39; NJ&#39;, 1)]</div>"]}}],"execution_count":40},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":41}],"metadata":{"name":"Spark_RDDs","notebookId":2659105190132887},"nbformat":4,"nbformat_minor":0}
