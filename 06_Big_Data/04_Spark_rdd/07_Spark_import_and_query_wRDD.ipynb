{"cells":[{"cell_type":"markdown","source":["#Big Data Management\nDatabricks + Spark  \nGarth Mortensen  \n2020.03.06\n\nRender markdown with [%md](https://forums.databricks.com/answers/9506/view.html)\n\n## Tasks\n\n2. Filter out header rows\n3. Provide an output that has employee count by department ID\n4. Provide an output that has the top two department names with the most employee ID.\n\n## Execution\n\n### Read files as RDDs\n\nWe want to read the files in as RDDs. \n\nFirst, I upload the two files to Databricks cluster, then use shell to see if the files are in the right place."],"metadata":{}},{"cell_type":"code","source":["%sh\ncd /dbfs/FileStore/tables\nls -la"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 0\ndrwxr-xr-x 1 root root        0 Jan  1  1970 .\ndrwxr-xr-x 1 root root        0 Jan  1  1970 ..\n-rw-r--r-- 1 root root 12859382 Mar 22 14:33 CAND_MSI_UNI_JS.json\n-rw-r--r-- 1 root root     5139 Mar  7 02:55 cities.txt\n-rw-r--r-- 1 root root 18513237 Mar 22 14:32 CMTE_MST_UNI_JS.json\n-rw-r--r-- 1 root root 60930417 Mar 22 14:34 CONTRIB_TO_CMTE_FR_CMTE_MN_JS.json\n-rw-r--r-- 1 root root      107 Mar 21 17:51 Dept1.csv\n-rw-r--r-- 1 root root      107 Mar  8 14:43 Dept.csv\n-rw-r--r-- 1 root root      714 Mar 21 17:51 Emp.csv\n-rw-r--r-- 1 root root      243 Mar 21 17:51 Emp.json\n-rw-r--r-- 1 root root      664 Mar  8 14:43 Employee.csv\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["Because I have read permission, I don't need to change permissions.\n\n```sh\n%sh\ncd /dbfs/FileStore/tables\nls -la\n```\n\nGood. The files are ready. Now time to figure out how to read them. Of note, I'm importing these without headers."],"metadata":{}},{"cell_type":"markdown","source":["### Read File\n\n#### Pandas DataFrame"],"metadata":{}},{"cell_type":"code","source":["# the pandas solution\nimport pandas as pd\n\ndept_pd_df = pd.read_csv(\"/dbfs/FileStore/tables/Dept.csv\", header=0)\n\nemp_pd_df = pd.read_csv(\"/dbfs/FileStore/tables/Employee.csv\", header=0)\nemp_pd_df[0:5]  # print first 5 rows"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>empno</th>\n      <th>ename</th>\n      <th>job</th>\n      <th>mgr</th>\n      <th>hiredate</th>\n      <th>sal</th>\n      <th>comm</th>\n      <th>deptno</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7369</td>\n      <td>SMITH</td>\n      <td>CLERK</td>\n      <td>7902.0</td>\n      <td>6/13/1993</td>\n      <td>800</td>\n      <td>0.0</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7499</td>\n      <td>ALLEN</td>\n      <td>SALESMAN</td>\n      <td>7698.0</td>\n      <td>8/15/1998</td>\n      <td>1600</td>\n      <td>300.0</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7521</td>\n      <td>WARD</td>\n      <td>SALESMAN</td>\n      <td>7698.0</td>\n      <td>3/26/1996</td>\n      <td>1250</td>\n      <td>500.0</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7566</td>\n      <td>JONES</td>\n      <td>MANAGER</td>\n      <td>7839.0</td>\n      <td>10/31/1995</td>\n      <td>2975</td>\n      <td>NaN</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7698</td>\n      <td>BLAKE</td>\n      <td>MANAGER</td>\n      <td>7839.0</td>\n      <td>6/11/1992</td>\n      <td>2850</td>\n      <td>NaN</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Spark DataFrame"],"metadata":{}},{"cell_type":"code","source":["# the spark solution\n\n# Define path and read csv without headers\nfile_emp = \"/FileStore/tables/Employee.csv\"\nemp_df = spark.read.csv(file_emp, header=\"true\", inferSchema=\"true\")\n\nfile_dept = \"/FileStore/tables/Dept.csv\"\ndept_df = spark.read.csv(file_dept, header=\"true\", inferSchema=\"true\")\ndept_df.take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: [Row(deptno=10, dname=&#39;Accounting&#39;, location=&#39;New York&#39;),\n Row(deptno=20, dname=&#39;Research&#39;, location=&#39;Dallas&#39;),\n Row(deptno=30, dname=&#39;Sales&#39;, location=&#39;Chicago&#39;)]</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["So I can import as pandas df or as spark df. Why would I choose one over the other? \n\nI'm guessing that because pandas doesn't distribute info out and in to the cluster. I bet only spark does that."],"metadata":{}},{"cell_type":"markdown","source":["### Spark Textfile"],"metadata":{}},{"cell_type":"code","source":["employee_txt = sc.textFile(\"/FileStore/tables/Employee.csv\")\nemployee_txt.take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[66]: [&#39;empno,ename,job,mgr,hiredate,sal,comm,deptno&#39;,\n &#39;7369,SMITH,CLERK,7902,6/13/1993,800,0,20&#39;,\n &#39;7499,ALLEN,SALESMAN,7698,8/15/1998,1600,300,30&#39;,\n &#39;7521,WARD,SALESMAN,7698,3/26/1996,1250,500,30&#39;,\n &#39;7566,JONES,MANAGER,7839,10/31/1995,2975,,20&#39;]</div>"]}}],"execution_count":10},{"cell_type":"code","source":["dept_txt = sc.textFile(\"/FileStore/tables/Dept.csv\")\ndept_txt.take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[67]: [&#39;deptno,dname,location&#39;,\n &#39;10,Accounting,New York&#39;,\n &#39;20,Research,Dallas&#39;,\n &#39;30,Sales,Chicago&#39;,\n &#39;40,Operations,Boston&#39;]</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["#### Spark Remove Headers"],"metadata":{}},{"cell_type":"code","source":["header = employee_txt.first()  # pull first row\nemployee_txt = employee_txt.filter(lambda row: row != header)\nemployee_txt.take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[68]: [&#39;7369,SMITH,CLERK,7902,6/13/1993,800,0,20&#39;,\n &#39;7499,ALLEN,SALESMAN,7698,8/15/1998,1600,300,30&#39;,\n &#39;7521,WARD,SALESMAN,7698,3/26/1996,1250,500,30&#39;,\n &#39;7566,JONES,MANAGER,7839,10/31/1995,2975,,20&#39;,\n &#39;7698,BLAKE,MANAGER,7839,6/11/1992,2850,,30&#39;]</div>"]}}],"execution_count":13},{"cell_type":"code","source":["header = dept_txt.first()\ndept_txt = dept_txt.filter(lambda row: row != header)\ndept_txt.take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[69]: [&#39;10,Accounting,New York&#39;,\n &#39;20,Research,Dallas&#39;,\n &#39;30,Sales,Chicago&#39;,\n &#39;40,Operations,Boston&#39;]</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["### Group By Setup\n\nFind employee count by department ID. \n\nThis could be either a:\n1. [RDD Action](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions), which runs a computation on the dataset, then returns the value to the driver program.\n2. [RDD Transformation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations), which creates a new dataset from an existing one.\n\nreduceByKey (func, [numPartitions]) - When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- the sql solution\n-- select count(*)\n-- from emp_df\n-- group by deptno"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["#### Pandas DataFrame"],"metadata":{}},{"cell_type":"code","source":["# the pandas solution\nemp_pd_df.groupby(\"deptno\")[\"empno\"].count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: deptno\n10    3\n20    5\n30    6\nName: empno, dtype: int64</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Let's first preview our data."],"metadata":{}},{"cell_type":"markdown","source":["#### Spark DataFrame"],"metadata":{}},{"cell_type":"code","source":["# the spark solution\nemp_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- empno: integer (nullable = true)\n-- ename: string (nullable = true)\n-- job: string (nullable = true)\n-- mgr: integer (nullable = true)\n-- hiredate: string (nullable = true)\n-- sal: integer (nullable = true)\n-- comm: integer (nullable = true)\n-- deptno: integer (nullable = true)\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["emp_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+------+---------+----+----------+----+----+------+\nempno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n+-----+------+---------+----+----------+----+----+------+\n 7369| SMITH|    CLERK|7902| 6/13/1993| 800|   0|    20|\n 7499| ALLEN| SALESMAN|7698| 8/15/1998|1600| 300|    30|\n 7521|  WARD| SALESMAN|7698| 3/26/1996|1250| 500|    30|\n 7566| JONES|  MANAGER|7839|10/31/1995|2975|null|    20|\n 7698| BLAKE|  MANAGER|7839| 6/11/1992|2850|null|    30|\n 7782| CLARK|  MANAGER|7839| 5/14/1993|2450|null|    10|\n 7788| SCOTT|  ANALYST|7566|  3/5/1996|3000|null|    20|\n 7839|  KING|PRESIDENT|null|  6/9/1990|5000|   0|    10|\n 7844|TURNER| SALESMAN|7698|  6/4/1995|1500|   0|    30|\n 7876| ADAMS|    CLERK|7788|  6/4/1999|1100|null|    20|\n 7900| JAMES|    CLERK|7698| 6/23/2000| 950|null|    30|\n 7934|MILLER|    CLERK|7782| 1/21/2000|1300|null|    10|\n 7902|  FORD|  ANALYST|7566| 12/5/1997|3000|null|    20|\n 7654|MARTIN| SALESMAN|7698| 12/5/1998|1250|1400|    30|\n+-----+------+---------+----+----------+----+----+------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["emp_df.describe().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+-----+--------+------------------+---------+------------------+-----------------+------------------+\nsummary|             empno|ename|     job|               mgr| hiredate|               sal|             comm|            deptno|\n+-------+------------------+-----+--------+------------------+---------+------------------+-----------------+------------------+\n  count|                14|   14|      14|                13|       14|                14|                6|                14|\n   mean| 7726.571428571428| null|    null| 7739.307692307692|     null| 2073.214285714286|366.6666666666667|22.142857142857142|\n stddev|178.29436087795662| null|    null|103.71466033898376|     null|1182.5032235162716|546.5040408511785| 8.017837257372731|\n    min|              7369|ADAMS| ANALYST|              7566|1/21/2000|               800|                0|                10|\n    max|              7934| WARD|SALESMAN|              7902|8/15/1998|              5000|             1400|                30|\n+-------+------------------+-----+--------+------------------+---------+------------------+-----------------+------------------+\n\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["Now, I'll perform the Group By, and display the **top two** department names with the most employee IDs.\n\n#### Pandas DataFrame\n\nDo it in pandas."],"metadata":{}},{"cell_type":"code","source":["# the pandas solution\njoined = pd.merge(emp_pd_df, dept_pd_df, on='deptno', sort='deptno')\njoined"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>empno</th>\n      <th>ename</th>\n      <th>job</th>\n      <th>mgr</th>\n      <th>hiredate</th>\n      <th>sal</th>\n      <th>comm</th>\n      <th>deptno</th>\n      <th>dname</th>\n      <th>location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7782</td>\n      <td>CLARK</td>\n      <td>MANAGER</td>\n      <td>7839.0</td>\n      <td>5/14/1993</td>\n      <td>2450</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>Accounting</td>\n      <td>New York</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7839</td>\n      <td>KING</td>\n      <td>PRESIDENT</td>\n      <td>NaN</td>\n      <td>6/9/1990</td>\n      <td>5000</td>\n      <td>0.0</td>\n      <td>10</td>\n      <td>Accounting</td>\n      <td>New York</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7934</td>\n      <td>MILLER</td>\n      <td>CLERK</td>\n      <td>7782.0</td>\n      <td>1/21/2000</td>\n      <td>1300</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>Accounting</td>\n      <td>New York</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7369</td>\n      <td>SMITH</td>\n      <td>CLERK</td>\n      <td>7902.0</td>\n      <td>6/13/1993</td>\n      <td>800</td>\n      <td>0.0</td>\n      <td>20</td>\n      <td>Research</td>\n      <td>Dallas</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7566</td>\n      <td>JONES</td>\n      <td>MANAGER</td>\n      <td>7839.0</td>\n      <td>10/31/1995</td>\n      <td>2975</td>\n      <td>NaN</td>\n      <td>20</td>\n      <td>Research</td>\n      <td>Dallas</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7788</td>\n      <td>SCOTT</td>\n      <td>ANALYST</td>\n      <td>7566.0</td>\n      <td>3/5/1996</td>\n      <td>3000</td>\n      <td>NaN</td>\n      <td>20</td>\n      <td>Research</td>\n      <td>Dallas</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7876</td>\n      <td>ADAMS</td>\n      <td>CLERK</td>\n      <td>7788.0</td>\n      <td>6/4/1999</td>\n      <td>1100</td>\n      <td>NaN</td>\n      <td>20</td>\n      <td>Research</td>\n      <td>Dallas</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7902</td>\n      <td>FORD</td>\n      <td>ANALYST</td>\n      <td>7566.0</td>\n      <td>12/5/1997</td>\n      <td>3000</td>\n      <td>NaN</td>\n      <td>20</td>\n      <td>Research</td>\n      <td>Dallas</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7499</td>\n      <td>ALLEN</td>\n      <td>SALESMAN</td>\n      <td>7698.0</td>\n      <td>8/15/1998</td>\n      <td>1600</td>\n      <td>300.0</td>\n      <td>30</td>\n      <td>Sales</td>\n      <td>Chicago</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7521</td>\n      <td>WARD</td>\n      <td>SALESMAN</td>\n      <td>7698.0</td>\n      <td>3/26/1996</td>\n      <td>1250</td>\n      <td>500.0</td>\n      <td>30</td>\n      <td>Sales</td>\n      <td>Chicago</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>7698</td>\n      <td>BLAKE</td>\n      <td>MANAGER</td>\n      <td>7839.0</td>\n      <td>6/11/1992</td>\n      <td>2850</td>\n      <td>NaN</td>\n      <td>30</td>\n      <td>Sales</td>\n      <td>Chicago</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7844</td>\n      <td>TURNER</td>\n      <td>SALESMAN</td>\n      <td>7698.0</td>\n      <td>6/4/1995</td>\n      <td>1500</td>\n      <td>0.0</td>\n      <td>30</td>\n      <td>Sales</td>\n      <td>Chicago</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7900</td>\n      <td>JAMES</td>\n      <td>CLERK</td>\n      <td>7698.0</td>\n      <td>6/23/2000</td>\n      <td>950</td>\n      <td>NaN</td>\n      <td>30</td>\n      <td>Sales</td>\n      <td>Chicago</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7654</td>\n      <td>MARTIN</td>\n      <td>SALESMAN</td>\n      <td>7698.0</td>\n      <td>12/5/1998</td>\n      <td>1250</td>\n      <td>1400.0</td>\n      <td>30</td>\n      <td>Sales</td>\n      <td>Chicago</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["joined2 = joined.groupby(\"dname\")[\"empno\"].count().to_frame()  # group by, and convert output series back to df\njoined2.sort_values('empno', ascending=False)  # it's not perfect, but good enough. I want to see Sales and Research at the end"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>empno</th>\n    </tr>\n    <tr>\n      <th>dname</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sales</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>Research</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Accounting</th>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["#### Spark DataFrame"],"metadata":{}},{"cell_type":"code","source":["emp_df.groupBy(\"deptno\").count().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----+\ndeptno|count|\n+------+-----+\n    20|    5|\n    10|    3|\n    30|    6|\n+------+-----+\n\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["This doesn't capture the department name. I'll need to join for that. No fancy footwork needed on the join. An inner will do the job, since there won't be anyone in the employees table without a department, we don't care about left joining."],"metadata":{}},{"cell_type":"code","source":["df_joined = emp_df.join(dept_df, on='deptno', how='inner')  # inner is default.\ndf_joined.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----+------+---------+----+----------+----+----+----------+--------+\ndeptno|empno| ename|      job| mgr|  hiredate| sal|comm|     dname|location|\n+------+-----+------+---------+----+----------+----+----+----------+--------+\n    20| 7369| SMITH|    CLERK|7902| 6/13/1993| 800|   0|  Research|  Dallas|\n    30| 7499| ALLEN| SALESMAN|7698| 8/15/1998|1600| 300|     Sales| Chicago|\n    30| 7521|  WARD| SALESMAN|7698| 3/26/1996|1250| 500|     Sales| Chicago|\n    20| 7566| JONES|  MANAGER|7839|10/31/1995|2975|null|  Research|  Dallas|\n    30| 7698| BLAKE|  MANAGER|7839| 6/11/1992|2850|null|     Sales| Chicago|\n    10| 7782| CLARK|  MANAGER|7839| 5/14/1993|2450|null|Accounting|New York|\n    20| 7788| SCOTT|  ANALYST|7566|  3/5/1996|3000|null|  Research|  Dallas|\n    10| 7839|  KING|PRESIDENT|null|  6/9/1990|5000|   0|Accounting|New York|\n    30| 7844|TURNER| SALESMAN|7698|  6/4/1995|1500|   0|     Sales| Chicago|\n    20| 7876| ADAMS|    CLERK|7788|  6/4/1999|1100|null|  Research|  Dallas|\n    30| 7900| JAMES|    CLERK|7698| 6/23/2000| 950|null|     Sales| Chicago|\n    10| 7934|MILLER|    CLERK|7782| 1/21/2000|1300|null|Accounting|New York|\n    20| 7902|  FORD|  ANALYST|7566| 12/5/1997|3000|null|  Research|  Dallas|\n    30| 7654|MARTIN| SALESMAN|7698| 12/5/1998|1250|1400|     Sales| Chicago|\n+------+-----+------+---------+----+----------+----+----+----------+--------+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["df_joined2 = df_joined.groupBy(\"dname\").count()\ndf_joined2.show()  # .show() on a variable assignment results in Type: None. Need to break it into 2 parts."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----+\n     dname|count|\n+----------+-----+\n     Sales|    6|\nAccounting|    3|\n  Research|    5|\n+----------+-----+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["df_joined2.orderBy('count', ascending=False).show(2)  # show only top 2 records\n\n# alternative\n# df_joined2.sort('count').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-----+\n   dname|count|\n+--------+-----+\n   Sales|    6|\nResearch|    5|\n+--------+-----+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["And one last curiosity was to check the filesystem for the temp tables I've created. After hunting around for a few minutes, I couldn't locate any."],"metadata":{}},{"cell_type":"code","source":["%sh\n\ncd /dbfs/\nls -la"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ls: cannot access &#39;databricks-results&#39;: No such file or directory\ntotal 8\ndrwxrwxrwx 1 root root    4096 Jan  1  1970 .\ndrwxr-xr-x 1 root root    4096 Mar 29 12:34 ..\n-rw-r--r-- 1 root root 4610348 Mar 21 19:00 bank-full.csv\ndrwxr-xr-x 1 root root       0 Jan  1  1970 databricks-datasets\n?????????? ? ?    ?          ?            ? databricks-results\ndrwxr-xr-x 1 root root       0 Jan  1  1970 FileStore\ndrwxrwxrwx 2 root root    4096 Mar 29 12:34 ml\ndrwxr-xr-x 1 root root       0 Jan  1  1970 tmp\ndrwxr-xr-x 1 root root       0 Jan  1  1970 user\n</div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["#### Spark RDD"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4344670797617315&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>header <span class=\"ansi-blue-fg\">=</span> employee_txt<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># pull first row</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> employee1 <span class=\"ansi-blue-fg\">=</span> employee_txt<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> row<span class=\"ansi-blue-fg\">:</span> row <span class=\"ansi-blue-fg\">!=</span> header<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> employee1<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">5</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1417</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1418</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1419</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1420</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1421</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1399</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1400</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1401</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1402</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1403</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1176</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1177</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1178</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1179</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1180</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 134.0 failed 1 times, most recent failure: Lost task 0.0 in stage 134.0 (TID 868, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 470, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2542, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2542, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 353, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1904, in combineLocally\n    merger.mergeValues(iterator)\n  File &#34;/databricks/spark/python/pyspark/shuffle.py&#34;, line 238, in mergeValues\n    for k, v in iterator:\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-4344670797617313&gt;&#34;, line 2, in &lt;lambda&gt;\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$collectPartitions(PythonRDD.scala:186)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:205)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor414.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 470, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2542, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2542, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 353, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1904, in combineLocally\n    merger.mergeValues(iterator)\n  File &#34;/databricks/spark/python/pyspark/shuffle.py&#34;, line 238, in mergeValues\n    for k, v in iterator:\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-4344670797617313&gt;&#34;, line 2, in &lt;lambda&gt;\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["emp1 = employee_txt.map(lambda x: x.split(','))\nemp2 = emp1.map(lambda x: (x[7],1))\nemp3 = emp2.reduceByKey(lambda x, y: x + y)\nemp3.take(5)  # this is not working."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[76]: [(&#39;20&#39;, 5), (&#39;10&#39;, 3), (&#39;deptno&#39;, 1), (&#39;30&#39;, 6)]</div>"]}}],"execution_count":37},{"cell_type":"code","source":["join_rdd = employee_txt.leftOuterJoin(dept_txt)\njoin_rdd.take(5)  # no good!"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[77]: [(&#39;e&#39;, (&#39;m&#39;, None)),\n (&#39;7&#39;, (&#39;3&#39;, None)),\n (&#39;7&#39;, (&#39;4&#39;, None)),\n (&#39;7&#39;, (&#39;5&#39;, None)),\n (&#39;7&#39;, (&#39;5&#39;, None))]</div>"]}}],"execution_count":38},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":39}],"metadata":{"name":"Spark_import_and_query","notebookId":4344670797617273},"nbformat":4,"nbformat_minor":0}
