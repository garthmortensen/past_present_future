{"cells":[{"cell_type":"markdown","source":["#Big Data Management\nDatabricks + Spark. File 2.  \n2020.03.06\n\nRender markdown with %md"],"metadata":{}},{"cell_type":"markdown","source":["### map(func)\nReturn a new distributed dataset formed by passing each element of the source through a function func."],"metadata":{}},{"cell_type":"code","source":["# Create an RDD\nrdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# apply map(func) transformation to the RDD\nrdd1 = rdd.map(lambda x: x * 5 + 1)\n\n# show results of the new rdd\nrdd1.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: [6, 11, 16, 21, 26]</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["### flatmap(func)\n\nSimilar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."],"metadata":{}},{"cell_type":"code","source":["# Create RDD using sample data\nrdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# apply map(func) transformation to the RDD\nrdd2 = rdd.flatMap(lambda x: [x, x * 3])\n\n# show results of a new rdd\nrdd2.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: [1, 3, 2, 6, 3, 9, 4, 12, 5, 15]</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Filter\nReturn a new dataset formed by selecting those elements of the source on which func returns true."],"metadata":{}},{"cell_type":"code","source":["# Create RDD using sample data\nrdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# apply filter(func) transformation to the RDD\nrdd.filter(lambda x: x % 2 == 0).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [2, 4]</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Key-value pairs\n\nA Key/Value RDD is an RDD whose elements comprise a pair of values – key and value. It should be in a tuple format such as (1,2) and then you apply key-value pair operations . For example: join(), groupByKey(), or reduceByKey()"],"metadata":{}},{"cell_type":"code","source":["# Setup the textFile RDD to read the README.md file\n# Note: this is lazy\ntextFile = sc.textFile(\"databricks-datasets/samples/docs/README.md\")\n\n# split each line of readme file to words first, and then make a tuple of (word, 1)\ntextFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[17]: [(&#39;Welcome&#39;, 1)]</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### reduceByKey\n\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."],"metadata":{}},{"cell_type":"code","source":["# Setup the textFile RDD to read the README.md file\ntextFile = sc.textFile(\"databricks-datasets/samples/docs/README.md\")\n\n# split each line of readme file to words first, and then make a tuple of word, 1\nrdd_key = textFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n\n# reduce by key\n\nrdd_key.reduceByKey(lambda x, y: x + y).take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: [(&#39;Welcome&#39;, 1), (&#39;Spark&#39;, 9), (&#39;documentation!&#39;, 1)]</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["### union\n\nReturn a new dataset that contains the union of the elements in the source dataset and the argument."],"metadata":{}},{"cell_type":"code","source":["# create some rdds\nrdd1 = sc.parallelize([1, 2, 3, 4, 5])\nrdd2 = rdd1.map(lambda x: x * 2)\n\n# combine these rdds with a union\nrdd1.union(rdd2).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[19]: [1, 2, 3, 4, 5, 2, 4, 6, 8, 10]</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["### groupByKey()\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance."],"metadata":{}},{"cell_type":"code","source":["# Setup the textFile RDD to read the README.md file\ntextFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n\n# split each line of the readme file to words, and then make a tuple of word, 1.\nrdd_key = textFile.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n\n# group by key\nrdd_gp = rdd_key.groupByKey()\n\nfor (key, value) in rdd_gp.take(5):\n  print(key, sum(value))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Welcome 1\nSpark 9\ndocumentation! 1\n 71\nreadme 1\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["### join\nWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin."],"metadata":{}},{"cell_type":"code","source":["# create two RRDs\nrdd1 = sc.parallelize([('rock', 1), ('paper', 2), ('scissor', 1), ('hammer', 3)])\nrdd2 = sc.parallelize([('hammer', 2), ('paper', 3), ('water', 1), ('fire', 3)])\n\n# perform left outer join\nrdd2.leftOuterJoin(rdd1).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[26]: [(&#39;paper&#39;, (3, 2)),\n (&#39;hammer&#39;, (2, 3)),\n (&#39;water&#39;, (1, None)),\n (&#39;fire&#39;, (3, None))]</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### stats()\n\nReturn the count, mean, standard deviation, max and min of the RDDs' elements in one operation."],"metadata":{}},{"cell_type":"code","source":["# stats transformation to get mean, count, std dev\nsc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9]).stats()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: (count: 9, mean: 5.0, stdev: 2.581988897471611, max: 9.0, min: 1.0)</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["### sample(withReplacement, fraction, seed=None)\n\nSample a fraction fraction of the data, with or without replacement, using a given random number generator seed.  \nParameters:\t\nwithReplacement – can elements be sampled multiple times (replaced when sampled out)\nfraction – expected size of the sample as a fraction of this RDD’s size without\nseed – seed for the random number generator"],"metadata":{}},{"cell_type":"code","source":["# Setup the textFile RDD to read the README.md file\ntextFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n\n# split each line of the readme file to words first, then make a truple of the word.\nrdd_key = textFile.flatMap(lambda x: x.split(' '))\nrdd_key.sample(False, 0.02, 3).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[33]: [&#39;in&#39;,\n &#39;the&#39;,\n &#39;the&#39;,\n &#39;the&#39;,\n &#39;which&#39;,\n &#39;while)&#39;,\n &#39;&#39;,\n &#39;site&#39;,\n &#39;Documentation&#39;,\n &#39;SPARK_PROJECT_ROOT/R/create-docs.sh.&#39;,\n &#39;various&#39;]</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["### Functions \nCreate a function and use it for a transformation."],"metadata":{}},{"cell_type":"code","source":["# create a function that tells if the line is small or large.\n\ndef strLenType(input):\n  if len(input) < 15:\n    return \"Small\"\n  else:\n    return \"Large\"\n\n# Setup the textFile RDD to read the README.md file\ntextFile = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n\n# split each line of the readme to words, then tuple-ize them\ntextFile.map(lambda x: strLenType(x)).take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[34]: [&#39;Large&#39;, &#39;Small&#39;, &#39;Large&#39;, &#39;Large&#39;, &#39;Large&#39;]</div>"]}}],"execution_count":23},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Spark_RDD_Transformations","notebookId":2659105190132903},"nbformat":4,"nbformat_minor":0}
